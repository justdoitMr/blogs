<!doctype html>
<html lang="zh-CN" data-theme="light">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <meta name="generator" content="VuePress 2.0.0-rc.12" />
    <meta name="theme" content="VuePress Theme Hope 2.0.0-rc.47" />
    <style>
      html {
        background: var(--bg-color, #fff);
      }

      html[data-theme="dark"] {
        background: var(--bg-color, #1d1e1f);
      }

      body {
        background: var(--bg-color);
      }
    </style>
    <script>
      const userMode = localStorage.getItem("vuepress-theme-hope-scheme");
      const systemDarkMode =
        window.matchMedia &&
        window.matchMedia("(prefers-color-scheme: dark)").matches;

      if (userMode === "dark" || (userMode !== "light" && systemDarkMode)) {
        document.documentElement.setAttribute("data-theme", "dark");
      }
    </script>
    <meta property="og:url" content="https://www.bugcode.online/ContainerCloud/k8s/act_none_pod%E5%BC%82%E5%B8%B8%E7%8A%B6%E6%80%81%E6%8E%92%E6%9F%A5.html"><meta property="og:site_name" content="bugcode 的架构之路"><meta property="og:title" content="2、Pod异常状态排错"><meta property="og:description" content="2、Pod异常状态排错 一、常用命令 首先列出Pod排查过程中我这边的常用命令： 查看Pod状态：kubectl get pod podname -o wide 查看Pod的yaml配置：kubectl get pods podname -o yaml 查看pod事件：kubectl describe pods podname 查看容器日志：kubec..."><meta property="og:type" content="article"><meta property="og:locale" content="zh-CN"><meta property="og:updated_time" content="2024-08-12T08:08:43.000Z"><meta property="article:author" content="bugcode"><meta property="article:tag" content="DOCKER"><meta property="article:tag" content="云原生"><meta property="article:tag" content="K8S"><meta property="article:published_time" content="2020-01-01T00:00:00.000Z"><meta property="article:modified_time" content="2024-08-12T08:08:43.000Z"><script type="application/ld+json">{"@context":"https://schema.org","@type":"Article","headline":"2、Pod异常状态排错","image":[""],"datePublished":"2020-01-01T00:00:00.000Z","dateModified":"2024-08-12T08:08:43.000Z","author":[{"@type":"Person","name":"bugcode"}]}</script><meta name="robots" content="all"><meta name="author" content="bugcode"><meta http-equiv="Cache-Control" content="no-cache, no-store, must-revalidate"><meta http-equiv="Pragma" content="no-cache"><meta http-equiv="Expires" content="0"><meta name="keywords" content="Java, Java基础, 并发编程, JVM, 虚拟机, 数据库, MySQL, Spring, Redis, MyBatis, SpringBoot, IDEA, 求职面试, 面渣逆袭, 学习路线"><meta name="apple-mobile-web-app-capable" content="yes"><script>
        var _hmt = _hmt || [];
        (function() {
          var hm = document.createElement("script");
          hm.src = "https://hm.baidu.com/hm.js?5230ac143650bf5eb3c14f3fb9b1d3ec";
          var s = document.getElementsByTagName("script")[0]; 
          s.parentNode.insertBefore(hm, s);
        })();
      </script><link rel="stylesheet" href="//at.alicdn.com/t/c/font_4570931_i3acfllzyt.css"><link rel="manifest" href="/manifest.webmanifest" crossorigin="use-credentials"><meta name="theme-color" content="#46bd87"><title>2、Pod异常状态排错 | bugcode 的架构之路</title><meta name="description" content="2、Pod异常状态排错 一、常用命令 首先列出Pod排查过程中我这边的常用命令： 查看Pod状态：kubectl get pod podname -o wide 查看Pod的yaml配置：kubectl get pods podname -o yaml 查看pod事件：kubectl describe pods podname 查看容器日志：kubec...">
    <link rel="stylesheet" href="/assets/css/styles.6b04e72b.css">
    <link rel="preload" href="/assets/js/runtime~app.9637422e.js" as="script"><link rel="preload" href="/assets/css/styles.6b04e72b.css" as="style"><link rel="preload" href="/assets/js/9547.c8ff98a9.js" as="script"><link rel="preload" href="/assets/js/app.03f81d61.js" as="script">
    
  </head>
  <body>
    <div id="app"><!--[--><!--[--><!--[--><span tabindex="-1"></span><a href="#main-content" class="vp-skip-link sr-only">跳至主要內容</a><!--]--><div class="theme-container external-link-icon has-toc"><!--[--><header id="navbar" class="vp-navbar"><div class="vp-navbar-start"><button type="button" class="vp-toggle-sidebar-button" title="Toggle Sidebar"><span class="icon"></span></button><!----><!--[--><a class="route-link vp-brand" href="/"><img class="vp-nav-logo" src="https://vscodepic.oss-cn-beijing.aliyuncs.com/blog/log.jpg" alt><!----><span class="vp-site-name hide-in-pad">bugcode 的架构之路</span></a><!--]--><!----></div><div class="vp-navbar-center"><!----><!--[--><nav class="vp-nav-links"><div class="vp-nav-item hide-in-mobile"><a class="route-link auto-link" href="/" aria-label="主页"><!--[--><span class="font-icon icon iconfont icon-home" style=""></span><!--]-->主页<!----></a></div><div class="vp-nav-item hide-in-mobile"><a class="route-link auto-link" href="/introduction/" aria-label="导读"><!--[--><span class="font-icon icon iconfont icon-book" style=""></span><!--]-->导读<!----></a></div><div class="vp-nav-item hide-in-mobile"><div class="vp-dropdown-wrapper"><button type="button" class="vp-dropdown-title" aria-label="算法"><!--[--><span class="font-icon icon iconfont icon-pen-to-square" style=""></span>算法<!--]--><span class="arrow"></span><ul class="vp-dropdown"><li class="vp-dropdown-item"><a class="route-link auto-link" href="/algorithm/algor/" aria-label="算法专题"><!--[--><span class="font-icon icon iconfont icon-pen-to-square" style=""></span><!--]-->算法专题<!----></a></li><li class="vp-dropdown-item"><a class="route-link auto-link" href="/algorithm/dataStructure/" aria-label="数据结构专题"><!--[--><span class="font-icon icon iconfont icon-pen-to-square" style=""></span><!--]-->数据结构专题<!----></a></li></ul></button></div></div><div class="vp-nav-item hide-in-mobile"><div class="vp-dropdown-wrapper"><button type="button" class="vp-dropdown-title" aria-label="设计模式"><!--[--><span class="font-icon icon iconfont icon-pen-to-square" style=""></span>设计模式<!--]--><span class="arrow"></span><ul class="vp-dropdown"><li class="vp-dropdown-item"><a class="route-link auto-link" href="/designpattern/designPrinciple/" aria-label="设计原则"><!--[--><span class="font-icon icon iconfont icon-pen-to-square" style=""></span><!--]-->设计原则<!----></a></li><li class="vp-dropdown-item"><a class="route-link auto-link" href="/designpattern/creational/" aria-label="创建型"><!--[--><span class="font-icon icon iconfont icon-pen-to-square" style=""></span><!--]-->创建型<!----></a></li><li class="vp-dropdown-item"><a class="route-link auto-link" href="/designpattern/Behavior/" aria-label="行为型"><!--[--><span class="font-icon icon iconfont icon-pen-to-square" style=""></span><!--]-->行为型<!----></a></li><li class="vp-dropdown-item"><a class="route-link auto-link" href="/designpattern/structural/" aria-label="结构型"><!--[--><span class="font-icon icon iconfont icon-pen-to-square" style=""></span><!--]-->结构型<!----></a></li></ul></button></div></div><div class="vp-nav-item hide-in-mobile"><div class="vp-dropdown-wrapper"><button type="button" class="vp-dropdown-title" aria-label="源码"><!--[--><span class="font-icon icon iconfont icon-pen-to-square" style=""></span>源码<!--]--><span class="arrow"></span><ul class="vp-dropdown"><li class="vp-dropdown-item"><a class="route-link auto-link" href="/frameDesignAndSourceCode/springSourceCode/" aria-label="Spring源码实现"><!--[--><span class="font-icon icon iconfont icon-pen-to-square" style=""></span><!--]-->Spring源码实现<!----></a></li><li class="vp-dropdown-item"><a class="route-link auto-link" href="/frameDesignAndSourceCode/mybatisSourceCode/" aria-label="Mybatis框架实现"><!--[--><span class="font-icon icon iconfont icon-pen-to-square" style=""></span><!--]-->Mybatis框架实现<!----></a></li></ul></button></div></div><div class="vp-nav-item hide-in-mobile"><div class="vp-dropdown-wrapper"><button type="button" class="vp-dropdown-title" aria-label="架构专题"><!--[--><span class="font-icon icon iconfont icon-pen-to-square" style=""></span>架构专题<!--]--><span class="arrow"></span><ul class="vp-dropdown"><li class="vp-dropdown-item"><a class="route-link auto-link" href="/engineeringArchitectureDesign/" aria-label="MVC和DDD"><!--[--><span class="font-icon icon iconfont icon-pen-to-square" style=""></span><!--]-->MVC和DDD<!----></a></li></ul></button></div></div><div class="vp-nav-item hide-in-mobile"><div class="vp-dropdown-wrapper"><button type="button" class="vp-dropdown-title" aria-label="精选博文"><!--[--><span class="font-icon icon iconfont icon-pen-to-square" style=""></span>精选博文<!--]--><span class="arrow"></span><ul class="vp-dropdown"><li class="vp-dropdown-item"><h4 class="vp-dropdown-subtitle">业务知识</h4><ul class="vp-dropdown-subitems"><li class="vp-dropdown-subitem"><a class="route-link auto-link" href="/excellentBook/business/" aria-label="金融业务"><!--[--><span class="font-icon icon iconfont icon-pen-to-square" style=""></span><!--]-->金融业务<!----></a></li></ul></li><li class="vp-dropdown-item"><h4 class="vp-dropdown-subtitle">项目经验</h4><ul class="vp-dropdown-subitems"><li class="vp-dropdown-subitem"><a class="route-link auto-link" href="/excellentBook/project/" aria-label="K8S部署项目"><!--[--><span class="font-icon icon iconfont icon-pen-to-square" style=""></span><!--]-->K8S部署项目<!----></a></li></ul></li><li class="vp-dropdown-item"><a class="route-link auto-link" href="/excellentBook/" aria-label="通用数据设计"><!--[--><span class="font-icon icon iconfont icon-pen-to-square" style=""></span><!--]-->通用数据设计<!----></a></li><li class="vp-dropdown-item"><a class="route-link auto-link" href="/excellentBook/scence.html" aria-label="业务场景设计"><!--[--><span class="font-icon icon iconfont icon-pen-to-square" style=""></span><!--]-->业务场景设计<!----></a></li></ul></button></div></div><div class="vp-nav-item hide-in-mobile"><a class="route-link auto-link" href="/home.html" aria-label="基础归档"><!--[--><span class="font-icon icon iconfont icon-gaishu" style=""></span><!--]-->基础归档<!----></a></div><div class="vp-nav-item hide-in-mobile"><a class="route-link auto-link" href="/interview/k8s/" aria-label="面试篇"><!--[--><span class="font-icon icon iconfont icon-gaishu" style=""></span><!--]-->面试篇<!----></a></div><div class="vp-nav-item hide-in-mobile"><a class="route-link auto-link" href="/KnowledgePlanet/" aria-label="知识星球"><!--[--><span class="font-icon icon iconfont icon-gaishu" style=""></span><!--]-->知识星球<!----></a></div><div class="vp-nav-item hide-in-mobile"><a class="route-link auto-link" href="/KnowledgePlanet/" aria-label="B站"><!--[--><span class="font-icon icon iconfont icon-gaishu" style=""></span><!--]-->B站<!----></a></div><div class="vp-nav-item hide-in-mobile"><a class="route-link auto-link" href="/about/" aria-label="关于我"><!--[--><span class="font-icon icon iconfont icon-book" style=""></span><!--]-->关于我<!----></a></div></nav><!--]--><!----></div><div class="vp-navbar-end"><!----><!--[--><!----><div class="vp-nav-item vp-action"><a class="vp-action-link" href="https://github.com/justdoitMr/blogs/" target="_blank" rel="noopener noreferrer" aria-label="GitHub"><svg xmlns="http://www.w3.org/2000/svg" class="icon github-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="github icon" name="github" style="width:1.25rem;height:1.25rem;vertical-align:middle;"><path d="M511.957 21.333C241.024 21.333 21.333 240.981 21.333 512c0 216.832 140.544 400.725 335.574 465.664 24.49 4.395 32.256-10.07 32.256-23.083 0-11.69.256-44.245 0-85.205-136.448 29.61-164.736-64.64-164.736-64.64-22.315-56.704-54.4-71.765-54.4-71.765-44.587-30.464 3.285-29.824 3.285-29.824 49.195 3.413 75.179 50.517 75.179 50.517 43.776 75.008 114.816 53.333 142.762 40.79 4.523-31.66 17.152-53.377 31.19-65.537-108.971-12.458-223.488-54.485-223.488-242.602 0-53.547 19.114-97.323 50.517-131.67-5.035-12.33-21.93-62.293 4.779-129.834 0 0 41.258-13.184 134.912 50.346a469.803 469.803 0 0 1 122.88-16.554c41.642.213 83.626 5.632 122.88 16.554 93.653-63.488 134.784-50.346 134.784-50.346 26.752 67.541 9.898 117.504 4.864 129.834 31.402 34.347 50.474 78.123 50.474 131.67 0 188.586-114.73 230.016-224.042 242.09 17.578 15.232 33.578 44.672 33.578 90.454v135.85c0 13.142 7.936 27.606 32.854 22.87C862.25 912.597 1002.667 728.747 1002.667 512c0-271.019-219.648-490.667-490.71-490.667z"></path></svg></a></div><div class="vp-nav-item hide-in-mobile"><button type="button" class="vp-outlook-button" tabindex="-1" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" class="icon outlook-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="outlook icon" name="outlook"><path d="M224 800c0 9.6 3.2 44.8 6.4 54.4 6.4 48-48 76.8-48 76.8s80 41.6 147.2 0 134.4-134.4 38.4-195.2c-22.4-12.8-41.6-19.2-57.6-19.2C259.2 716.8 227.2 761.6 224 800zM560 675.2l-32 51.2c-51.2 51.2-83.2 32-83.2 32 25.6 67.2 0 112-12.8 128 25.6 6.4 51.2 9.6 80 9.6 54.4 0 102.4-9.6 150.4-32l0 0c3.2 0 3.2-3.2 3.2-3.2 22.4-16 12.8-35.2 6.4-44.8-9.6-12.8-12.8-25.6-12.8-41.6 0-54.4 60.8-99.2 137.6-99.2 6.4 0 12.8 0 22.4 0 12.8 0 38.4 9.6 48-25.6 0-3.2 0-3.2 3.2-6.4 0-3.2 3.2-6.4 3.2-6.4 6.4-16 6.4-16 6.4-19.2 9.6-35.2 16-73.6 16-115.2 0-105.6-41.6-198.4-108.8-268.8C704 396.8 560 675.2 560 675.2zM224 419.2c0-28.8 22.4-51.2 51.2-51.2 28.8 0 51.2 22.4 51.2 51.2 0 28.8-22.4 51.2-51.2 51.2C246.4 470.4 224 448 224 419.2zM320 284.8c0-22.4 19.2-41.6 41.6-41.6 22.4 0 41.6 19.2 41.6 41.6 0 22.4-19.2 41.6-41.6 41.6C339.2 326.4 320 307.2 320 284.8zM457.6 208c0-12.8 12.8-25.6 25.6-25.6 12.8 0 25.6 12.8 25.6 25.6 0 12.8-12.8 25.6-25.6 25.6C470.4 233.6 457.6 220.8 457.6 208zM128 505.6C128 592 153.6 672 201.6 736c28.8-60.8 112-60.8 124.8-60.8-16-51.2 16-99.2 16-99.2l316.8-422.4c-48-19.2-99.2-32-150.4-32C297.6 118.4 128 291.2 128 505.6zM764.8 86.4c-22.4 19.2-390.4 518.4-390.4 518.4-22.4 28.8-12.8 76.8 22.4 99.2l9.6 6.4c35.2 22.4 80 12.8 99.2-25.6 0 0 6.4-12.8 9.6-19.2 54.4-105.6 275.2-524.8 288-553.6 6.4-19.2-3.2-32-19.2-32C777.6 76.8 771.2 80 764.8 86.4z"></path></svg><div class="vp-outlook-dropdown"><!----></div></button></div><!----><!--]--><!----><button type="button" class="vp-toggle-navbar-button" aria-label="Toggle Navbar" aria-expanded="false" aria-controls="nav-screen"><span><span class="vp-top"></span><span class="vp-middle"></span><span class="vp-bottom"></span></span></button></div></header><!----><!--]--><!----><div class="toggle-sidebar-wrapper"><span class="arrow start"></span></div><aside id="sidebar" class="vp-sidebar"><!----><ul class="vp-sidebar-links"><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">一、导读</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">二、java核心</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">三、Spring篇</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">四、大数据篇</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable active" type="button"><!----><span class="vp-sidebar-title">五、云原生</span><span class="vp-arrow down"></span></button><ul class="vp-sidebar-links"><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">1、Docker</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable active" type="button"><!----><span class="vp-sidebar-title">2、K8S</span><span class="vp-arrow down"></span></button><ul class="vp-sidebar-links"><li><a class="route-link auto-link vp-sidebar-link vp-sidebar-page" href="/ContainerCloud/k8s/act_one_k8s%E4%B8%AD%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E5%8E%9F%E7%90%86.md.html" aria-label="1、K8s中负载均衡原理"><!--[--><span class="font-icon icon iconfont icon-file" style=""></span><!--]-->1、K8s中负载均衡原理<!----></a></li><li><a class="route-link route-link-active auto-link vp-sidebar-link vp-sidebar-page active" href="/ContainerCloud/k8s/act_none_pod%E5%BC%82%E5%B8%B8%E7%8A%B6%E6%80%81%E6%8E%92%E6%9F%A5.html" aria-label="2、Pod异常状态排错"><!--[--><span class="font-icon icon iconfont icon-file" style=""></span><!--]-->2、Pod异常状态排错<!----></a></li><li><a class="route-link auto-link vp-sidebar-link vp-sidebar-page" href="/ContainerCloud/k8s/act_two_k8s%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0.html" aria-label="3、K8S基础学习"><!--[--><span class="font-icon icon iconfont icon-file" style=""></span><!--]-->3、K8S基础学习<!----></a></li></ul></section></li></ul></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">五、前端篇</span><span class="vp-arrow end"></span></button><!----></section></li></ul><!----></aside><!--[--><main id="main-content" class="vp-page"><!--[--><!----><!----><nav class="vp-breadcrumb disable"></nav><div class="vp-page-title"><h1><span class="font-icon icon iconfont icon-file" style=""></span>2、Pod异常状态排错</h1><div class="page-info"><span class="page-author-info" aria-label="作者🖊" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon author-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="author icon" name="author"><path d="M649.6 633.6c86.4-48 147.2-144 147.2-249.6 0-160-128-288-288-288s-288 128-288 288c0 108.8 57.6 201.6 147.2 249.6-121.6 48-214.4 153.6-240 288-3.2 9.6 0 19.2 6.4 25.6 3.2 9.6 12.8 12.8 22.4 12.8h704c9.6 0 19.2-3.2 25.6-12.8 6.4-6.4 9.6-16 6.4-25.6-25.6-134.4-121.6-240-243.2-288z"></path></svg><span><span class="page-author-item">bugcode</span></span><span property="author" content="bugcode"></span></span><!----><span class="page-date-info" aria-label="写作日期📅" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon calendar-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="calendar icon" name="calendar"><path d="M716.4 110.137c0-18.753-14.72-33.473-33.472-33.473-18.753 0-33.473 14.72-33.473 33.473v33.473h66.993v-33.473zm-334.87 0c0-18.753-14.72-33.473-33.473-33.473s-33.52 14.72-33.52 33.473v33.473h66.993v-33.473zm468.81 33.52H716.4v100.465c0 18.753-14.72 33.473-33.472 33.473a33.145 33.145 0 01-33.473-33.473V143.657H381.53v100.465c0 18.753-14.72 33.473-33.473 33.473a33.145 33.145 0 01-33.473-33.473V143.657H180.6A134.314 134.314 0 0046.66 277.595v535.756A134.314 134.314 0 00180.6 947.289h669.74a134.36 134.36 0 00133.94-133.938V277.595a134.314 134.314 0 00-133.94-133.938zm33.473 267.877H147.126a33.145 33.145 0 01-33.473-33.473c0-18.752 14.72-33.473 33.473-33.473h736.687c18.752 0 33.472 14.72 33.472 33.473a33.145 33.145 0 01-33.472 33.473z"></path></svg><span><!----></span><meta property="datePublished" content="2020-01-01T00:00:00.000Z"></span><!----><span class="page-reading-time-info" aria-label="阅读时间⌛" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon timer-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="timer icon" name="timer"><path d="M799.387 122.15c4.402-2.978 7.38-7.897 7.38-13.463v-1.165c0-8.933-7.38-16.312-16.312-16.312H256.33c-8.933 0-16.311 7.38-16.311 16.312v1.165c0 5.825 2.977 10.874 7.637 13.592 4.143 194.44 97.22 354.963 220.201 392.763-122.204 37.542-214.893 196.511-220.2 389.397-4.661 5.049-7.638 11.651-7.638 19.03v5.825h566.49v-5.825c0-7.379-2.849-13.981-7.509-18.9-5.049-193.016-97.867-351.985-220.2-389.527 123.24-37.67 216.446-198.453 220.588-392.892zM531.16 450.445v352.632c117.674 1.553 211.787 40.778 211.787 88.676H304.097c0-48.286 95.149-87.382 213.728-88.676V450.445c-93.077-3.107-167.901-81.297-167.901-177.093 0-8.803 6.99-15.793 15.793-15.793 8.803 0 15.794 6.99 15.794 15.793 0 80.261 63.69 145.635 142.01 145.635s142.011-65.374 142.011-145.635c0-8.803 6.99-15.793 15.794-15.793s15.793 6.99 15.793 15.793c0 95.019-73.789 172.82-165.96 177.093z"></path></svg><span>大约 19 分钟</span><meta property="timeRequired" content="PT19M"></span><span class="page-category-info" aria-label="分类🌈" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon category-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="category icon" name="category"><path d="M148.41 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H148.41c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.311-40.31zM147.556 553.478H429.73c22.263 0 40.311 18.048 40.311 40.31v282.176c0 22.263-18.048 40.312-40.31 40.312H147.555c-22.263 0-40.311-18.049-40.311-40.312V593.79c0-22.263 18.048-40.311 40.31-40.311zM593.927 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H593.927c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.31-40.31zM730.22 920.502H623.926c-40.925 0-74.22-33.388-74.22-74.425V623.992c0-41.038 33.387-74.424 74.425-74.424h222.085c41.038 0 74.424 33.226 74.424 74.067v114.233c0 10.244-8.304 18.548-18.547 18.548s-18.548-8.304-18.548-18.548V623.635c0-20.388-16.746-36.974-37.33-36.974H624.13c-20.585 0-37.331 16.747-37.331 37.33v222.086c0 20.585 16.654 37.331 37.126 37.331H730.22c10.243 0 18.547 8.304 18.547 18.547 0 10.244-8.304 18.547-18.547 18.547z"></path></svg><!--[--><span class="page-category-item color0 clickable" role="navigation">K8S</span><span class="page-category-item color2 clickable" role="navigation">DOCKER</span><span class="page-category-item color5 clickable" role="navigation">JAVA</span><!--]--><meta property="articleSection" content="K8S,DOCKER,JAVA"></span><span class="page-tag-info" aria-label="标签🏷" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon tag-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="tag icon" name="tag"><path d="M939.902 458.563L910.17 144.567c-1.507-16.272-14.465-29.13-30.737-30.737L565.438 84.098h-.402c-3.215 0-5.726 1.005-7.634 2.913l-470.39 470.39a10.004 10.004 0 000 14.164l365.423 365.424c1.909 1.908 4.42 2.913 7.132 2.913s5.223-1.005 7.132-2.913l470.39-470.39c2.01-2.11 3.014-5.023 2.813-8.036zm-240.067-72.121c-35.458 0-64.286-28.828-64.286-64.286s28.828-64.285 64.286-64.285 64.286 28.828 64.286 64.285-28.829 64.286-64.286 64.286z"></path></svg><!--[--><span class="page-tag-item color2 clickable" role="navigation">DOCKER</span><span class="page-tag-item color5 clickable" role="navigation">云原生</span><span class="page-tag-item color0 clickable" role="navigation">K8S</span><!--]--><meta property="keywords" content="DOCKER,云原生,K8S"></span></div><hr></div><div class="vp-toc-placeholder"><aside id="toc"><!----><div class="vp-toc-header">此页内容<button type="button" class="print-button" title="打印"><svg xmlns="http://www.w3.org/2000/svg" class="icon print-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="print icon" name="print"><path d="M819.2 364.8h-44.8V128c0-17.067-14.933-32-32-32H281.6c-17.067 0-32 14.933-32 32v236.8h-44.8C145.067 364.8 96 413.867 96 473.6v192c0 59.733 49.067 108.8 108.8 108.8h44.8V896c0 17.067 14.933 32 32 32h460.8c17.067 0 32-14.933 32-32V774.4h44.8c59.733 0 108.8-49.067 108.8-108.8v-192c0-59.733-49.067-108.8-108.8-108.8zM313.6 160h396.8v204.8H313.6V160zm396.8 704H313.6V620.8h396.8V864zM864 665.6c0 25.6-19.2 44.8-44.8 44.8h-44.8V588.8c0-17.067-14.933-32-32-32H281.6c-17.067 0-32 14.933-32 32v121.6h-44.8c-25.6 0-44.8-19.2-44.8-44.8v-192c0-25.6 19.2-44.8 44.8-44.8h614.4c25.6 0 44.8 19.2 44.8 44.8v192z"></path></svg></button><div class="arrow end"></div></div><div class="vp-toc-wrapper"><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#一、常用命令">一、常用命令</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#二、pod状态">二、Pod状态</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#三、pod遇到的问题">三、pod遇到的问题</a></li><li><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#_1、pod一直处于pending状态">1、pod一直处于Pending状态</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#_2、pod-一直处于-containercreating-或-waiting-状-态">2、Pod 一直处于 ContainerCreating 或 Waiting 状 态</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#_3、pod-处于-crashloopbackoff-状态">3、Pod 处于 CrashLoopBackOff 状态</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#_4、pod-一直处于-terminating-状态">4、Pod 一直处于 Terminating 状态</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#_5、pod-一直处于-unknown-状态">5、Pod 一直处于 Unknown 状态</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#_6、pod-一直处于-error-状态">6、Pod 一直处于 Error 状态</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#_7、pod-一直处于-imagepullbackoff-状态">7、Pod 一直处于 ImagePullBackOff 状态</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#_8、pod-健康检查失败">8、Pod 健康检查失败</a></li><!----><!--]--></ul></li><!--]--></ul><div class="vp-toc-marker" style="top:-1.7rem;"></div></div><!----></aside></div><!----><div class="theme-hope-content"><p>2、Pod异常状态排错</p><h2 id="一、常用命令" tabindex="-1"><a class="header-anchor" href="#一、常用命令"><span>一、常用命令</span></a></h2><p>首先列出Pod排查过程中我这边的常用命令：</p><ul><li>查看Pod状态：kubectl get pod podname -o wide</li><li>查看Pod的yaml配置：kubectl get pods podname -o yaml</li><li>查看pod事件：kubectl describe pods podname</li><li>查看容器日志：kubectl logs podsname -c container-name</li></ul><h2 id="二、pod状态" tabindex="-1"><a class="header-anchor" href="#二、pod状态"><span>二、Pod状态</span></a></h2><ul><li>Error：Pod 启动过程中发生错误</li><li>NodeLost : Pod 所在节点失联</li><li>Unkown : Pod 所在节点失联或其它未知异常</li><li>Waiting : Pod 等待启动</li><li>Pending : Pod 等待被调度</li><li>ContainerCreating : Pod 容器正在被创建</li><li>Terminating : Pod 正在被销毁</li><li>CrashLoopBackOff ： 容器退出，kubelet 正在将它重启</li><li>InvalidImageName ： 无法解析镜像名称</li><li>ImageInspectError ： 无法校验镜像</li><li>ErrImageNeverPull ： 策略禁止拉取镜像</li><li>ImagePullBackOff ： 正在重试拉取</li><li>RegistryUnavailable ： 连接不到镜像中心</li><li>ErrImagePull ： 通用的拉取镜像出错</li><li>CreateContainerConfigError ： 不能创建 kubelet 使用的容器配置</li><li>CreateContainerError ： 创建容器失败</li><li>RunContainerError ： 启动容器失败</li><li>PreStartHookError : 执行 preStart hook 报错</li><li>PostStartHookError ： 执行 postStart hook 报错</li><li>ContainersNotInitialized ： 容器没有初始化完毕</li><li>ContainersNotReady ： 容器没有准备完毕</li><li>ContainerCreating ：容器创建中</li><li>PodInitializing ：pod 初始化中</li><li>DockerDaemonNotReady ：docker还没有完全启动</li><li>NetworkPluginNotReady ： 网络插件还没有完全启动</li></ul><h2 id="三、pod遇到的问题" tabindex="-1"><a class="header-anchor" href="#三、pod遇到的问题"><span>三、pod遇到的问题</span></a></h2><h3 id="_1、pod一直处于pending状态" tabindex="-1"><a class="header-anchor" href="#_1、pod一直处于pending状态"><span>1、pod一直处于Pending状态</span></a></h3><p>Pending 状态说明 Pod 还没有被调度到某个节点上，需要看下 Pod 事件进一步判断原因，比如:</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" data-title="" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span>$ kubectl describe pod tikv-0 </span></span>
<span class="line"><span>     . ... </span></span>
<span class="line"><span>       Events: </span></span>
<span class="line"><span>        Type     Reason   Age     From   Message </span></span>
<span class="line"><span>         ----    ------   ----    ----    ------- </span></span>
<span class="line"><span>        Warning FailedScheduling 3m (x106 over 33m) default-scheduler 0/4 nodes are available: 1 node(s) had no available volume zone, 2 Insufficient cpu, 3 Insufficient memory.</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>下面是我遇到的一些原因：</p><ul><li><strong>节点资源不够</strong>：节点资源不够有以下几种情况: <ul><li>CPU负载过高</li><li>剩余可被分配的内存不足</li><li>剩余可用GPU数量不足</li></ul></li></ul><p>如何判断某个 Node 资源是否足够？ 通过下面的命令查看node资源情况，关注以下信息：</p><div class="language-java line-numbers-mode" data-highlighter="shiki" data-ext="java" data-title="java" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E06C75;">kubectl describe node nodename</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><ul><li>Allocatable : 表示此节点能够申请的资源总和</li><li>Allocated resources : 表示此节点已分配的资源 (Allocatable 减去节点上所有 Pod 总 的 Request)</li></ul><p>前者与后者相减，可得出剩余可申请的资源。如果这个值小于 Pod 的 request，就不满足 Pod 的 资源要求，Scheduler 在 Predicates (预选) 阶段就会剔除掉这个 Node，也就不会调度上去。</p><ul><li><strong>不满足 nodeSelector 与 affinity</strong></li></ul><p>如果 Pod 包含 nodeSelector 指定了节点需要包含的 label，调度器将只会考虑将 Pod 调度到 包含这些 label 的Node 上，如果没有 Node 有这些 label 或者有这些 label 的 Node 其它 条件不满足也将会无法调度。参考官方文档<br><a href="https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector" target="_blank" rel="noopener noreferrer">https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector</a><br> 如果 Pod 包含 affinity（亲和性）的配置，调度器根据调度算法也可能算出没有满足条件的 Node，从而无法调度。affinity 有以下几类:</p><ul><li>nodeAffinity: 节点亲和性，可以看成是增强版的 nodeSelector，用于限制 Pod 只允许 被调度到某一部分 Node。</li><li>podAffinity: Pod 亲和性，用于将一些有关联的 Pod 调度到同一个地方，同一个地方可以 是指同一个节点或同一个可用区的节点等。</li><li>podAntiAffinity: Pod 反亲和性，用于避免将某一类 Pod 调度到同一个地方避免单点故 障，比如将集群 DNS 服务的 Pod 副本都调度到不同节点，避免一个节点挂了造成整个集群 DNS 解析失败，使得业务中断。</li></ul><p><strong>Node 存在 Pod 没有容忍的污点</strong><br> 如果节点上存在污点 (Taints)，而 Pod 没有响应的容忍 (Tolerations)，Pod 也将不会调度上 去。通过 describe node 可以看下 Node 有哪些 Taints:</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" data-title="" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span>$ kubectl describe nodes host1 </span></span>
<span class="line"><span>    ... </span></span>
<span class="line"><span>    Taints: special=true:NoSchedule</span></span>
<span class="line"><span>    ...</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>污点既可以是手动添加也可以是被自动添加，下面可以看一下。<br><strong>手动添加的污点：</strong><br> 通过类似以下方式可以给节点添加污点:</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" data-title="" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span>$ kubectl taint node host1 special=true:NoSchedule </span></span>
<span class="line"><span>  node &quot;host1&quot; tainted</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div></div></div><p>另外，有些场景下希望新加的节点默认不调度 Pod，直到调整完节点上某些配置才允许调度，就给新加 <a href="http://xn--node-z94fz6v820d7xjpj0agd6a.kubernetes.io/unschedulable" target="_blank" rel="noopener noreferrer">的节点都加上node.kubernetes.io/unschedulable</a> 这个污点。<br><strong>自动添加的污点</strong><br> 如果节点运行状态不正常，污点也可以被自动添加，从 v1.12 开始， TaintNodesByCondition 特性进入 Beta 默认开启，controller manager 会检查 Node 的 Condition，如果命中条件 就自动为 Node 加上相应的污点，这些 Condition 与 Taints 的对应关系如下:</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" data-title="" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span>Conditon Value Taints </span></span>
<span class="line"><span> -------- ----- ------</span></span>
<span class="line"><span> OutOfDisk True node.kubernetes.io/out-of-disk</span></span>
<span class="line"><span> Ready False node.kubernetes.io/not-ready</span></span>
<span class="line"><span> Ready Unknown node.kubernetes.io/unreachable</span></span>
<span class="line"><span> MemoryPressure True node.kubernetes.io/memory-pressure</span></span>
<span class="line"><span> PIDPressure True node.kubernetes.io/pid-pressure</span></span>
<span class="line"><span> DiskPressure True node.kubernetes.io/disk-pressure</span></span>
<span class="line"><span> NetworkUnavailable True node.kubernetes.io/network-unavailable</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>解释下上面各种条件的意思:</p><ul><li>OutOfDisk 为 True 表示节点磁盘空间不够了</li><li>Ready 为 False 表示节点不健康</li><li>Ready 为 Unknown 表示节点失联，在 node-monitor-grace-period 这么长的时间内没有 上报状态 controller-manager 就会将 Node 状态置为 Unknown (默认 40s)</li><li>MemoryPressure 为 True 表示节点内存压力大，实际可用内存很少</li><li>PIDPressure 为 True 表示节点上运行了太多进程，PID 数量不够用了</li><li>DiskPressure 为 True 表示节点上的磁盘可用空间太少了</li><li>NetworkUnavailable 为 True 表示节点上的网络没有正确配置，无法跟其它 Pod 正常通 信</li></ul><p>另外，在云环境下，比如腾讯云 TKE，添加新节点会先给这个 Node 加上<br><a href="http://node.cloudprovider.kubernetes.io/uninitialized" target="_blank" rel="noopener noreferrer">node.cloudprovider.kubernetes.io/uninitialized</a> 的污点，等 Node 初始化成功后才自动移 除这个污点，避免 Pod 被调度到没初始化好的 Node 上。<br><strong>低版本 kube-scheduler 的 bug</strong><br> 可能是低版本 kube-scheduler 的 bug, 可以升级下调度器版本。<br><strong>kube-scheduler 没有正常运行</strong><br> 检查 maser 上的 kube-scheduler 是否运行正常，异常的话可以尝试重启临时恢复。<br><strong>驱逐后其它可用节点与当前节点有状态应用不在同一个可用区</strong></p><p>有时候服务部署成功运行过，但在某个时候节点突然挂了，此时就会触发驱逐，创建新的副本调度到其 它节点上，对于已经挂载了磁盘的 Pod，它通常需要被调度到跟当前节点和磁盘在同一个可用区，如果 集群中同一个可用区的节点不满足调度条件，即使其它可用区节点各种条件都满足，但不跟当前节点在 同一个可用区，也是不会调度的。为什么需要限制挂载了磁盘的 Pod 不能漂移到其它可用区的节点？ 试想一下，云上的磁盘虽然可以被动态挂载到不同机器，但也只是相对同一个<a href="https://so.csdn.net/so/search?q=%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%BF%83&amp;spm=1001.2101.3001.7020" target="_blank" rel="noopener noreferrer">数据中心</a>，通常不允许跨数据中心挂载磁盘设备，因为网络时延会极大的降低 IO 速率。</p><h3 id="_2、pod-一直处于-containercreating-或-waiting-状-态" tabindex="-1"><a class="header-anchor" href="#_2、pod-一直处于-containercreating-或-waiting-状-态"><span>2、Pod 一直处于 ContainerCreating 或 Waiting 状 态</span></a></h3><ul><li><strong>Pod 配置错误</strong><ul><li>检查是否打包了正确的镜像</li><li>检查配置了正确的容器参数</li></ul></li><li><strong>挂载 Volume 失败</strong><ul><li>Volume 挂载失败也分许多种情况，先列下我这里目前已知的。</li></ul></li></ul><p><strong>Pod 漂移没有正常解挂之前的磁盘</strong></p><blockquote><p>在云尝试托管的 <a href="https://so.csdn.net/so/search?q=K8S&amp;spm=1001.2101.3001.7020" target="_blank" rel="noopener noreferrer">K8S</a> 服务环境下，默认挂载的 Volume 一般是块存储类型的云硬盘，如果某个节点 挂了，kubelet 无法正常运行或与 apiserver 通信，到达时间阀值后会触发驱逐，自动在其它节点 上启动相同的副本 (Pod 漂移)，但是由于被驱逐的 Node 无法正常运行并不知道自己被驱逐了，也 就没有正常执行解挂，cloud-controller-manager 也在等解挂成功后再调用云厂商的接口将磁盘 真正从节点上解挂，通常会等到一个时间阀值后 cloud-controller-manager 会强制解挂云盘， 然后再将其挂载到 Pod 最新所在节点上，这种情况下 ContainerCreating 的时间相对长一点，但 一般最终是可以启动成功的，除非云厂商的 cloud-controller-manager 逻辑有 bug。</p></blockquote><p><strong>磁盘爆满</strong><br> 启动 Pod 会调 CRI 接口创建容器，容器运行时创建容器时通常会在数据目录下为新建的容器创建一 些目录和文件，如果数据目录所在的磁盘空间满了就会创建失败并报错:</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" data-title="" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span>Events：</span></span>
<span class="line"><span>    Type Reason Age From</span></span>
<span class="line"><span>Message</span></span>
<span class="line"><span>     ---- ------ ---- ----</span></span>
<span class="line"><span> Warning FailedCreatePodSandBox 2m (x4307 over 16h) kubelet, 10.179.80.31 (combined from similar events): Failed create pod sandbox: rpc error: code = Unknown desc = failed to create a sandbox for pod &quot;apigateway-6dc48bf8b6-l8xrw&quot;: Error response from daemon: mkdir /var/lib/docker/aufs/mnt/1f09d6c1c9f24e8daaea5bf33a4230de7dbc758e3b22785e8ee21e3e3d921214 no space left on device</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>处理方法: <a href="https://blog.csdn.net/qq_31055683/article/details/126974456" target="_blank" rel="noopener noreferrer">https://blog.csdn.net/qq_31055683/article/details/126974456</a><br><strong>节点内存碎片化</strong><br> 如果节点上内存碎片化严重，缺少大页内存，会导致即使总的剩余内存较多，但还是会申请内存失败，处理方法请看：<a href="https://blog.csdn.net/qq_31055683/article/details/126974873" target="_blank" rel="noopener noreferrer">https://blog.csdn.net/qq_31055683/article/details/12697487</a><br><strong>limit 设置太小或者单位不对</strong><br> 如果 limit 设置过小以至于不足以成功运行 Sandbox 也会造成这种状态，常见的是因为 memory limit 单位设置不对造成的 limit 过小，比如误将 memory 的 limit 单位像 request 一样设 置为小 <strong>m</strong> ，这个单位在 memory 不适用，会被 k8s 识别成 byte， 应该用 <strong>Mi</strong> 或<strong>M</strong><br><em>举个例子: 如果 memory limit 设为 1024m 表示限制 1.024 Byte，这么小的内存， pause 容器一起来就会被 cgroup-oom kill 掉，导致 pod 状态一直处于 ContainerCreating。</em><br> 这种情况通常会报下面的 event:</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" data-title="" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span>Pod sandbox changed, it will be killed and re-created。</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>kubelet 报错:</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" data-title="" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span>to start sandbox container for pod ... Error response from daemon: OCI runtime create failed: container_linux.go:348: starting container process caused &quot;process_linux.go:301: running exec setns process for init caused \&quot;signal: killed\&quot;&quot;: unknown</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p><strong>拉取镜像失败</strong><br> 镜像拉取失败也分很多情况，这里列举下:</p><ul><li>配置了错误的镜像</li><li>Kubelet 无法访问镜像仓库（比如默认 pause 镜像在 <a href="http://gcr.io" target="_blank" rel="noopener noreferrer">gcr.io</a> 上，国内环境访问需要特殊处 理）</li><li>拉取私有镜像的 imagePullSecret 没有配置或配置有误</li><li>镜像太大，拉取超时（可以适当调整 kubelet 的 —image-pull-progress-deadline 和 —runtime-request-timeout 选项）</li></ul><p><strong>CNI 网络错误</strong><br> 如果发生 CNI 网络错误通常需要检查下网络插件的配置和运行状态，如果没有正确配置或正常运行通 常表现为:</p><ul><li>无法配置 Pod 网络</li><li>无法分配 Pod IP</li></ul><p><strong>controller-manager 异常</strong><br> 查看 master 上 kube-controller-manager 状态，异常的话尝试重启。<br><strong>安装 docker 没删干净旧版本</strong></p><p>如果节点上本身有 docker 或者没删干净，然后又安装 docker，比如在 centos 上用 yum 安装:</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" data-title="" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span>yum install -y docker</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>这样可能会导致 dockerd 创建容器一直不成功，从而 Pod 状态一直 ContainerCreating，查看 event 报错:</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" data-title="" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span>Type     Reason        Age       From</span></span>
<span class="line"><span>Message</span></span>
<span class="line"><span>      ----    ------       ----       ----</span></span>
<span class="line"><span>      Warning FailedCreatePodSandBox 18m (x3583 over 83m) kubelet, 192.168.4.5 (combined from similar events): Failed create pod sandbox: rpc error: code = Unknown desc = failed to start sandbox container for pod &quot;nginx-7db9fccd9b-2j6dh&quot;: Error response from daemon: ttrpc: client shutting down: read unix @-&gt;@/containerd- shim/moby/de2bfeefc999af42783115acca62745e6798981dff75f4148fae8c086668f667/shim.sock</span></span>
<span class="line"><span> read: connection reset by peer: unknown</span></span>
<span class="line"><span> Normal SandboxChanged 3m12s (x4420 over 83m) kubelet, 192.168.4.5</span></span>
<span class="line"><span> Pod sandbox changed, it will be killed and re-created</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>可能是因为重复安装 docker 版本不一致导致一些组件之间不兼容，从而导致 dockerd 无法正常创 建容器。</p><ul><li><strong>存在同名容器</strong></li></ul><p>如果节点上已有同名容器，创建 sandbox 就会失败，event:</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" data-title="" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span>Warning FailedCreatePodSandBox 2m kubelet, 10.205.8.91 Failed create pod sandbox: rpc error: code = Unknown desc = failed to create a sandbox for pod &quot;lomp-ext-d8c8b8c46-4v8tl&quot;: operation timeout: context deadline exceeded</span></span>
<span class="line"><span>Warning FailedCreatePodSandBox 3s (x12 over 2m) kubelet, 10.205.8.91 Failed create pod sandbox: rpc error: code = Unknown desc = failed to create a sandbox for pod &quot;lomp-ext-d8c8b8c46-4v8tl&quot;: Error response from daemon: Conflict. The container name &quot;/k8s_POD_lomp-ext-d8c8b8c46- 4v8tl_default_65046a06-f795-11e9-9bb6-b67fb7a70bad_0&quot; is already in use by container &quot;30aa3f5847e0ce89e9d411e76783ba14accba7eb7743e605a10a9a862a72c1e2&quot;. You have to remove (or rename) that container to be able to reuse that name.</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div></div></div><p>关于什么情况下会产生同名容器，这个有待研究。</p><h3 id="_3、pod-处于-crashloopbackoff-状态" tabindex="-1"><a class="header-anchor" href="#_3、pod-处于-crashloopbackoff-状态"><span>3、Pod 处于 CrashLoopBackOff 状态</span></a></h3><p>Pod 如果处于 CrashLoopBackOff 状态说明之前是启动了，只是又异常退出了，只要 Pod 的 restartPolicy不是 Never 就可能被重启拉起，此时 Pod 的 RestartCounts 通常是大于 0 的，可以先看下容器进程的退出状态码来缩小问题范围</p><ul><li><strong>容器进程主动退出：</strong></li></ul><hr><p>如果是容器进程主动退出，退出状态码一般在 0-128 之间，除了可能是业务程序 BUG，还有其它许 多可能原因</p><ul><li><strong>系统OOM</strong></li></ul><hr><p>如果发生系统 OOM，可以看到 Pod 中容器退出状态码是 137，表示被 SIGKILL 信号杀死，同时 内核会报错: Out of memory: Kill process … 。大概率是节点上部署了其它非 K8S 管理的进 程消耗了比较多的内存，或者 kubelet 的 --kube-reserved 和 --system-reserved 配的 比较小，没有预留足够的空间给其它非容器进程，节点上所有 Pod 的实际内存占用总量不会超过 /sys/fs/cgroup/memory/kubepods 这里 cgroup 的限制，这个限制等于 capacity - &quot;kube- reserved&quot; - &quot;system-reserved&quot; ，如果预留空间设置合理，节点上其它非容器进程（kubelet, dockerd, kube-proxy, sshd 等) 内存占用没有超过 kubelet 配置的预留空间是不会发生系统 OOM 的，可以根据实际需求做合理的调整。</p><ul><li><strong>系统OOM</strong></li></ul><hr><p>如果是 cgrou OOM 杀掉的进程，从 Pod 事件的下 Reason 可以看到是 OOMKilled ，说明 容器实际占用的内存超过 limit 了，同时内核日志会报: ``。 可以根据需求调整下 limit。</p><ul><li><strong>节点内存碎片化</strong></li></ul><hr><p>如果节点上内存碎片化严重，缺少大页内存，会导致即使总的剩余内存较多，但还是会申请内存失败，</p><ul><li><strong>健康检查失败</strong></li></ul><hr><h3 id="_4、pod-一直处于-terminating-状态" tabindex="-1"><a class="header-anchor" href="#_4、pod-一直处于-terminating-状态"><span>4、Pod 一直处于 Terminating 状态</span></a></h3><ul><li><strong>磁盘爆满</strong></li></ul><hr><p>如果 docker 的数据目录所在磁盘被写满，docker 无法正常运行，无法进行删除和创建操作，所以 kubelet 调用 docker 删除容器没反应，看 event 类似这样：</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" data-title="" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span>Normal Killing 39s (x735 over 15h) kubelet, 10.179.80.31 Killing container with id docker://apigateway:Need to kill Pod</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><ul><li><strong>存在 “i” 文件属性</strong></li></ul><hr><p>如果容器的镜像本身或者容器启动后写入的文件存在 “i” 文件属性，此文件就无法被修改删除，而删 除 Pod 时会清理容器目录，但里面包含有不可删除的文件，就一直删不了，Pod 状态也将一直保持 Terminating，kubelet 报错:</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" data-title="" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span>Sep 27 14:37:21 VM_0_7_centos kubelet[14109]: E0927 14:37:21.922965 14109 remote_runtime RemoveContainer &quot;19d837c77a3c294052a99ff9347c520bc8acb7b8b9a9dc9fab281fc09df38257&quot; failed: rpc error: code = Unknown desc = failed to remove container &quot;19d837c77a3c294052a99ff9347c520bc8acb7b8b9a9dc9fab281fc09df38257&quot;: Error response 19d837c77a3c294052a99ff9347c520bc8acb7b8b9a9dc9fab281fc09df38257: driver &quot;overlay2&quot; filesystem: remove /data/docker/overlay2/b1aea29c590aa9abda79f7cf3976422073fb3652757f0391db88534027546868 operation not permitted</span></span>
<span class="line"><span>Sep 27 14:37:21 VM_0_7_centos kubelet[14109]: E0927 14:37:21.923027 14109 kuberuntime_gc to remove container &quot;19d837c77a3c294052a99ff9347c520bc8acb7b8b9a9dc9fab281fc09df38257&quot; Unknown desc = failed to remove container &quot;19d837c77a3c294052a99ff9347c520bc8acb7b8b9a9dc9fab281fc09df38257&quot;: Error response 19d837c77a3c294052a99ff9347c520bc8acb7b8b9a9dc9fab281fc09df38257: driver &quot;overlay2&quot; filesystem: remove /data/docker/overlay2/b1aea29c590aa9abda79f7cf3976422073fb3652757f0391db88534027546868 operation not permitted</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div></div></div><p>通过 <strong>man chattr</strong> 查看 “i” 文件属性描述:</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" data-title="" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span>A file with the &#39;i&#39; attribute cannot be modified: it cannot be deleted or renamed, no link can be created to this file and no data can be written to the file. Only the superuser or a process possessing the CAP_LINUX_IMMUTABLE capability can set or clear this attribute.</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>彻底解决当然是不要在容器镜像中或启动后的容器设置 “i” 文件属性，临时恢复方法： 复制 kubelet 日志报错提示的文件路径，然后执行 chattr -i 文件名 :</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" data-title="" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span>chattr -i /data/docker/overlay2/b1aea29c590aa9abda79f7cf3976422073fb3652757f0391db88534027</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>执行完后等待 kubelet 自动重试，Pod 就可以被自动删除了。</p><ul><li><strong>存在 Finalizers</strong></li></ul><hr><p>k8s 资源的 metadata 里如果存在 finalizers ，那么该资源一般是由某程序创建的，并且在其 创建的资源的 metadata 里的 finalizers 加了一个它的标识，这意味着这个资源被删除时需要 由创建资源的程序来做删除前的清理，清理完了它需要将标识从该资源的 finalizers 中移除，然 后才会最终彻底删除资源。比如 Rancher 创建的一些资源就会写入 finalizers 标识。<br> 处理建议： kubectl edit 手动编辑资源定义，删掉 finalizers ，这时再看下资源，就会发现 已经删掉了</p><h3 id="_5、pod-一直处于-unknown-状态" tabindex="-1"><a class="header-anchor" href="#_5、pod-一直处于-unknown-状态"><span>5、Pod 一直处于 Unknown 状态</span></a></h3><p>通常是节点失联，没有上报状态给 apiserver，到达阀值后 controller-manager 认为节点失联 并将其状态置为 Unknown<br> 可能原因:</p><ul><li>节点高负载导致无法上报</li><li>节点宕机</li><li>节点被关机</li><li>网络不通</li></ul><h3 id="_6、pod-一直处于-error-状态" tabindex="-1"><a class="header-anchor" href="#_6、pod-一直处于-error-状态"><span>6、Pod 一直处于 Error 状态</span></a></h3><p>通常处于 Error 状态说明 Pod 启动过程中发生了错误。常见的原因包括：</p><ul><li>依赖的 ConfigMap、Secret 或者 PV 等不存在</li><li>请求的资源超过了管理员设置的限制，比如超过了 LimitRange 等</li><li>违反集群的安全策略，比如违反了 PodSecurityPolicy 等</li><li>容器无权操作集群内的资源，比如开启 RBAC 后，需要为 ServiceAccount 配置角色绑定</li></ul><h3 id="_7、pod-一直处于-imagepullbackoff-状态" tabindex="-1"><a class="header-anchor" href="#_7、pod-一直处于-imagepullbackoff-状态"><span>7、Pod 一直处于 ImagePullBackOff 状态</span></a></h3><ul><li><strong>http 类型 registry，地址未加入到 insecure- registry</strong></li></ul><hr><p>dockerd 默认从 https 类型的 registry 拉取镜像，如果使用 https 类型的 registry，则 必须将它添加到 insecure-registry 参数中，然后重启或 reload dockerd 生效。</p><ul><li><strong>https 自签发类型 resitry，没有给节点添加 ca 证书</strong></li></ul><hr><p>如果 registry 是 https 类型，但证书是自签发的，dockerd 会校验 registry 的证书，校验 成功才能正常使用镜像仓库，要想校验成功就需要将 registry 的 ca 证书放置到<br> /etc/docker/certs.d/<a href="registry:port" target="_blank" rel="noopener noreferrer">registry:port</a>/ca.crt 位置</p><ul><li><strong>私有镜像仓库认证失败</strong></li></ul><hr><p>如果 registry 需要认证，但是 Pod 没有配置 imagePullSecret，配置的 Secret 不存在或者 有误都会认证失败。</p><ul><li><strong>镜像文件损坏</strong></li></ul><hr><p>如果 push 的镜像文件损坏了，下载下来也用不了，需要重新 push 镜像文件。</p><ul><li><strong>镜像拉取超时</strong></li></ul><hr><p>如果节点上新起的 Pod 太多就会有许多可能会造成容器镜像下载排队，如果前面有许多大镜像需要下 载很长时间，后面排队的 Pod 就会报拉取超时。<br> kubelet 默认串行下载镜像:</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" data-title="" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span>--serialize-image-pulls Pull images one at a time. We recommend *not* changing the default value on nodes that run docker daemon with version &lt; 1.9 or an Aufs storage backend. Issue #10959 has more details. (default true)</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>也可以开启并行下载并控制并发:</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" data-title="" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span>--registry-qps int32 If &gt; 0, limit registry pull QPS to this value. If 0, unlimited. (default 5)</span></span>
<span class="line"><span>--registry-burst int32 Maximum size of a bursty pulls, temporarily allows pulls to burst to this number, while still not exceeding registry-qps. Only used if --registry-qps &gt; 0 (default 10)</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div></div></div><ul><li><strong>镜像不存在</strong></li></ul><hr><p>kubelet日志：</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" data-title="" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span>PullImage &quot;imroc/test:v0.2&quot; from image service failed: rpc error: code = Unknown desc = Error response from daemon: manifest for imroc/test:v0.2 not found</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><h3 id="_8、pod-健康检查失败" tabindex="-1"><a class="header-anchor" href="#_8、pod-健康检查失败"><span>8、Pod 健康检查失败</span></a></h3><ul><li>Kubernetes 健康检查包含就绪检查(readinessProbe)和存活检查(livenessProbe)</li><li>pod 如果就绪检查失败会将此 pod ip 从 service 中摘除，通过 service 访问，流量将 不会被转发给就绪检查失败的 pod</li><li>pod 如果存活检查失败，kubelet 将会杀死容器并尝试重启</li></ul><p>健康检查失败的可能原因有多种，除了业务程序BUG导致不能响应健康检查导致 unhealthy，还能有 有其它原因，下面我们来逐个排查。</p><ul><li><strong>健康检查配置不合理</strong></li></ul><hr><p>initialDelaySeconds 太短，容器启动慢，导致容器还没完全启动就开始探测，如果 successThreshold 是默认值 1，检查失败一次就会被 kill，然后 pod 一直这样被 kill 重 启。</p><ul><li><strong>节点负载过高</strong></li></ul><hr><p>cpu 占用高（比如跑满）会导致进程无法正常发包收包，通常会 timeout，导致 kubelet 认为 pod 不健康</p><ul><li><strong>容器进程被木马进程杀死</strong></li></ul><hr><ul><li><strong>容器内进程端口监听挂掉</strong></li></ul><hr><p>使用 netstat -tunlp 检查端口监听是否还在，如果不在了，抓包可以看到会直接 reset 掉健 康检查探测的连接:</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" data-title="" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span>20:15:17.890996 IP 172.16.2.1.38074 &gt; 172.16.2.23.8888: Flags [S], seq 96880261, win 14600, options [mss 1424,nop,nop,sackOK,nop,wscale 7], length 0</span></span>
<span class="line"><span>20:15:17.891021 IP 172.16.2.23.8888 &gt; 172.16.2.1.38074: Flags [R.], seq 0, ack 96880262, win 0, length 0</span></span>
<span class="line"><span>20:15:17.906744 IP 10.0.0.16.54132 &gt; 172.16.2.23.8888: Flags [S], seq 1207014342, win 14600, options [mss 1424,nop,nop,sackOK,nop,wscale 7], length 0</span></span>
<span class="line"><span>20:15:17.906766 IP 172.16.2.23.8888 &gt; 10.0.0.16.54132: Flags [R.], seq 0, ack 1207014343, win 0, length 0</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>连接异常，从而健康检查失败。发生这种情况的原因可能在一个节点上启动了多个使用 hostNetwork 监听相同宿主机端口的 Pod，只会有一个 Pod 监听成功，但监听失败的 Pod 的 业务逻辑允许了监听失败，并没有退出，Pod 又配了健康检查，kubelet 就会给 Pod 发送健康检查 探测报文，但 Pod 由于没有监听所以就会健康检查失败。</p><ul><li><strong>SYN backlog 设置过小</strong></li></ul><hr><p>SYN backlog 大小即 SYN 队列大小，如果短时间内新建连接比较多，而 SYN backlog 设置太 小，就会导致新建连接失败，通过 netstat -s | grep TCPBacklogDrop可以看到有多少是因为 backlog 满了导致丢弃的新连接。<br> 如果确认是 backlog 满了导致的丢包，建议调高 backlog 的值，内核参数为 net.ipv4.tcp_max_syn_backlog 。</p><ul><li><strong>容器进程主动退出</strong></li></ul><hr><p>容器进程如果是自己主动退出(不是被外界中断杀死)，退出状态码一般在 0-128 之间，根据约定，正 常退出时状态码为 0，1-127 说明是程序发生异常，主动退出了，比如检测到启动的参数和条件不满 足要求，或者运行过程中发生 panic 但没有捕获处理导致程序退出。除了可能是业务程序 BUG，还 有其它许多可能原因，这里我们一一列举下。</p><ul><li><strong>DNS 无法解析</strong><br> 可能程序依赖 集群 DNS 服务，比如启动时连接数据库，数据库使用 service 名称或外部域名都需 要 DNS 解析，如果解析失败程序将报错并主动退出。解析失败的可能原因:1、集群网络有问题，Pod 连不上集群 DNS 服务<br> 2、集群 DNS 服务挂了，无法响应解析请求<br> 3、Service 或域名地址配置有误，本身是无法解析的地址</li><li><strong>程序配置有误</strong><br> 1、配置文件格式错误，程序启动解析配置失败报错退出<br> 2、配置内容不符合规范，比如配置中某个字段是必选但没有填写，配置校验不通过，程序报错主动 退出</li></ul><p>参考：<br><a href="https://blog.csdn.net/qq_31055683/article/details/126970831" target="_blank" rel="noopener noreferrer">https://blog.csdn.net/qq_31055683/article/details/126970831</a></p></div><!----><footer class="vp-page-meta"><div class="vp-meta-item edit-link"><a class="auto-link external-link vp-meta-label" href="https://github.com/justdoitMr/blogs//edit/master/themeHope/ContainerCloud/k8s/act_none_pod异常状态排查.md" aria-label="在 GitHub 上编辑此页" rel="noopener noreferrer" target="_blank"><!--[--><svg xmlns="http://www.w3.org/2000/svg" class="icon edit-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="edit icon" name="edit"><path d="M430.818 653.65a60.46 60.46 0 0 1-50.96-93.281l71.69-114.012 7.773-10.365L816.038 80.138A60.46 60.46 0 0 1 859.225 62a60.46 60.46 0 0 1 43.186 18.138l43.186 43.186a60.46 60.46 0 0 1 0 86.373L588.879 565.55l-8.637 8.637-117.466 68.234a60.46 60.46 0 0 1-31.958 11.229z"></path><path d="M728.802 962H252.891A190.883 190.883 0 0 1 62.008 771.98V296.934a190.883 190.883 0 0 1 190.883-192.61h267.754a60.46 60.46 0 0 1 0 120.92H252.891a69.962 69.962 0 0 0-69.098 69.099V771.98a69.962 69.962 0 0 0 69.098 69.098h475.911A69.962 69.962 0 0 0 797.9 771.98V503.363a60.46 60.46 0 1 1 120.922 0V771.98A190.883 190.883 0 0 1 728.802 962z"></path></svg><!--]-->在 GitHub 上编辑此页<!----></a></div><div class="vp-meta-item git-info"><div class="update-time"><span class="vp-meta-label">上次编辑于: </span><!----></div><div class="contributors"><span class="vp-meta-label">贡献者: </span><!--[--><!--[--><span class="vp-meta-info" title="email: 2284609302@qq.com">不爱打代码的程序员</span><!--]--><!--]--></div></div></footer><nav class="vp-page-nav"><a class="route-link auto-link prev" href="/ContainerCloud/k8s/act_one_k8s%E4%B8%AD%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E5%8E%9F%E7%90%86.md.html" aria-label="1、K8s中负载均衡原理"><div class="hint"><span class="arrow start"></span>上一页</div><div class="link"><span class="font-icon icon iconfont icon-file" style=""></span>1、K8s中负载均衡原理</div></a><a class="route-link auto-link next" href="/ContainerCloud/k8s/act_two_k8s%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0.html" aria-label="3、K8S基础学习"><div class="hint">下一页<span class="arrow end"></span></div><div class="link">3、K8S基础学习<span class="font-icon icon iconfont icon-file" style=""></span></div></a></nav><!----><!----><!--]--></main><!--]--><footer class="vp-footer-wrapper"><div class="vp-footer">云原生</div><div class="vp-copyright">bugcode</div></footer></div><!--]--><!--[--><!----><!----><!--]--><!--]--></div>
    <script src="/assets/js/runtime~app.9637422e.js" defer></script><script src="/assets/js/9547.c8ff98a9.js" defer></script><script src="/assets/js/app.03f81d61.js" defer></script>
  </body>
</html>
