"use strict";(self.webpackChunk=self.webpackChunk||[]).push([[4267],{6262:(e,i)=>{i.A=(e,i)=>{const n=e.__vccOpts||e;for(const[e,a]of i)n[e]=a;return n}},9640:(e,i,n)=>{n.r(i),n.d(i,{comp:()=>r,data:()=>t});var a=n(641);const s=[(0,a.Fv)('<p>2、Pod异常状态排错</p><h2 id="一、常用命令" tabindex="-1"><a class="header-anchor" href="#一、常用命令"><span>一、常用命令</span></a></h2><p>首先列出Pod排查过程中我这边的常用命令：</p><ul><li>查看Pod状态：kubectl get pod podname -o wide</li><li>查看Pod的yaml配置：kubectl get pods podname -o yaml</li><li>查看pod事件：kubectl describe pods podname</li><li>查看容器日志：kubectl logs podsname -c container-name</li></ul><h2 id="二、pod状态" tabindex="-1"><a class="header-anchor" href="#二、pod状态"><span>二、Pod状态</span></a></h2><ul><li>Error：Pod 启动过程中发生错误</li><li>NodeLost : Pod 所在节点失联</li><li>Unkown : Pod 所在节点失联或其它未知异常</li><li>Waiting : Pod 等待启动</li><li>Pending : Pod 等待被调度</li><li>ContainerCreating : Pod 容器正在被创建</li><li>Terminating : Pod 正在被销毁</li><li>CrashLoopBackOff ： 容器退出，kubelet 正在将它重启</li><li>InvalidImageName ： 无法解析镜像名称</li><li>ImageInspectError ： 无法校验镜像</li><li>ErrImageNeverPull ： 策略禁止拉取镜像</li><li>ImagePullBackOff ： 正在重试拉取</li><li>RegistryUnavailable ： 连接不到镜像中心</li><li>ErrImagePull ： 通用的拉取镜像出错</li><li>CreateContainerConfigError ： 不能创建 kubelet 使用的容器配置</li><li>CreateContainerError ： 创建容器失败</li><li>RunContainerError ： 启动容器失败</li><li>PreStartHookError : 执行 preStart hook 报错</li><li>PostStartHookError ： 执行 postStart hook 报错</li><li>ContainersNotInitialized ： 容器没有初始化完毕</li><li>ContainersNotReady ： 容器没有准备完毕</li><li>ContainerCreating ：容器创建中</li><li>PodInitializing ：pod 初始化中</li><li>DockerDaemonNotReady ：docker还没有完全启动</li><li>NetworkPluginNotReady ： 网络插件还没有完全启动</li></ul><h2 id="三、pod遇到的问题" tabindex="-1"><a class="header-anchor" href="#三、pod遇到的问题"><span>三、pod遇到的问题</span></a></h2><h3 id="_1、pod一直处于pending状态" tabindex="-1"><a class="header-anchor" href="#_1、pod一直处于pending状态"><span>1、pod一直处于Pending状态</span></a></h3><p>Pending 状态说明 Pod 还没有被调度到某个节点上，需要看下 Pod 事件进一步判断原因，比如:</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" data-title="" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span>$ kubectl describe pod tikv-0 </span></span>\n<span class="line"><span>     . ... </span></span>\n<span class="line"><span>       Events: </span></span>\n<span class="line"><span>        Type     Reason   Age     From   Message </span></span>\n<span class="line"><span>         ----    ------   ----    ----    ------- </span></span>\n<span class="line"><span>        Warning FailedScheduling 3m (x106 over 33m) default-scheduler 0/4 nodes are available: 1 node(s) had no available volume zone, 2 Insufficient cpu, 3 Insufficient memory.</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>下面是我遇到的一些原因：</p><ul><li><strong>节点资源不够</strong>：节点资源不够有以下几种情况: <ul><li>CPU负载过高</li><li>剩余可被分配的内存不足</li><li>剩余可用GPU数量不足</li></ul></li></ul><p>如何判断某个 Node 资源是否足够？ 通过下面的命令查看node资源情况，关注以下信息：</p><div class="language-java line-numbers-mode" data-highlighter="shiki" data-ext="java" data-title="java" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E06C75;">kubectl describe node nodename</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><ul><li>Allocatable : 表示此节点能够申请的资源总和</li><li>Allocated resources : 表示此节点已分配的资源 (Allocatable 减去节点上所有 Pod 总 的 Request)</li></ul><p>前者与后者相减，可得出剩余可申请的资源。如果这个值小于 Pod 的 request，就不满足 Pod 的 资源要求，Scheduler 在 Predicates (预选) 阶段就会剔除掉这个 Node，也就不会调度上去。</p><ul><li><strong>不满足 nodeSelector 与 affinity</strong></li></ul><p>如果 Pod 包含 nodeSelector 指定了节点需要包含的 label，调度器将只会考虑将 Pod 调度到 包含这些 label 的Node 上，如果没有 Node 有这些 label 或者有这些 label 的 Node 其它 条件不满足也将会无法调度。参考官方文档<br><a href="https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector" target="_blank" rel="noopener noreferrer">https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector</a><br> 如果 Pod 包含 affinity（亲和性）的配置，调度器根据调度算法也可能算出没有满足条件的 Node，从而无法调度。affinity 有以下几类:</p><ul><li>nodeAffinity: 节点亲和性，可以看成是增强版的 nodeSelector，用于限制 Pod 只允许 被调度到某一部分 Node。</li><li>podAffinity: Pod 亲和性，用于将一些有关联的 Pod 调度到同一个地方，同一个地方可以 是指同一个节点或同一个可用区的节点等。</li><li>podAntiAffinity: Pod 反亲和性，用于避免将某一类 Pod 调度到同一个地方避免单点故 障，比如将集群 DNS 服务的 Pod 副本都调度到不同节点，避免一个节点挂了造成整个集群 DNS 解析失败，使得业务中断。</li></ul><p><strong>Node 存在 Pod 没有容忍的污点</strong><br> 如果节点上存在污点 (Taints)，而 Pod 没有响应的容忍 (Tolerations)，Pod 也将不会调度上 去。通过 describe node 可以看下 Node 有哪些 Taints:</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" data-title="" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span>$ kubectl describe nodes host1 </span></span>\n<span class="line"><span>    ... </span></span>\n<span class="line"><span>    Taints: special=true:NoSchedule</span></span>\n<span class="line"><span>    ...</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>污点既可以是手动添加也可以是被自动添加，下面可以看一下。<br><strong>手动添加的污点：</strong><br> 通过类似以下方式可以给节点添加污点:</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" data-title="" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span>$ kubectl taint node host1 special=true:NoSchedule </span></span>\n<span class="line"><span>  node &quot;host1&quot; tainted</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div></div></div><p>另外，有些场景下希望新加的节点默认不调度 Pod，直到调整完节点上某些配置才允许调度，就给新加 <a href="http://xn--node-z94fz6v820d7xjpj0agd6a.kubernetes.io/unschedulable" target="_blank" rel="noopener noreferrer">的节点都加上node.kubernetes.io/unschedulable</a> 这个污点。<br><strong>自动添加的污点</strong><br> 如果节点运行状态不正常，污点也可以被自动添加，从 v1.12 开始， TaintNodesByCondition 特性进入 Beta 默认开启，controller manager 会检查 Node 的 Condition，如果命中条件 就自动为 Node 加上相应的污点，这些 Condition 与 Taints 的对应关系如下:</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" data-title="" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span>Conditon Value Taints </span></span>\n<span class="line"><span> -------- ----- ------</span></span>\n<span class="line"><span> OutOfDisk True node.kubernetes.io/out-of-disk</span></span>\n<span class="line"><span> Ready False node.kubernetes.io/not-ready</span></span>\n<span class="line"><span> Ready Unknown node.kubernetes.io/unreachable</span></span>\n<span class="line"><span> MemoryPressure True node.kubernetes.io/memory-pressure</span></span>\n<span class="line"><span> PIDPressure True node.kubernetes.io/pid-pressure</span></span>\n<span class="line"><span> DiskPressure True node.kubernetes.io/disk-pressure</span></span>\n<span class="line"><span> NetworkUnavailable True node.kubernetes.io/network-unavailable</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>解释下上面各种条件的意思:</p><ul><li>OutOfDisk 为 True 表示节点磁盘空间不够了</li><li>Ready 为 False 表示节点不健康</li><li>Ready 为 Unknown 表示节点失联，在 node-monitor-grace-period 这么长的时间内没有 上报状态 controller-manager 就会将 Node 状态置为 Unknown (默认 40s)</li><li>MemoryPressure 为 True 表示节点内存压力大，实际可用内存很少</li><li>PIDPressure 为 True 表示节点上运行了太多进程，PID 数量不够用了</li><li>DiskPressure 为 True 表示节点上的磁盘可用空间太少了</li><li>NetworkUnavailable 为 True 表示节点上的网络没有正确配置，无法跟其它 Pod 正常通 信</li></ul><p>另外，在云环境下，比如腾讯云 TKE，添加新节点会先给这个 Node 加上<br><a href="http://node.cloudprovider.kubernetes.io/uninitialized" target="_blank" rel="noopener noreferrer">node.cloudprovider.kubernetes.io/uninitialized</a> 的污点，等 Node 初始化成功后才自动移 除这个污点，避免 Pod 被调度到没初始化好的 Node 上。<br><strong>低版本 kube-scheduler 的 bug</strong><br> 可能是低版本 kube-scheduler 的 bug, 可以升级下调度器版本。<br><strong>kube-scheduler 没有正常运行</strong><br> 检查 maser 上的 kube-scheduler 是否运行正常，异常的话可以尝试重启临时恢复。<br><strong>驱逐后其它可用节点与当前节点有状态应用不在同一个可用区</strong></p><p>有时候服务部署成功运行过，但在某个时候节点突然挂了，此时就会触发驱逐，创建新的副本调度到其 它节点上，对于已经挂载了磁盘的 Pod，它通常需要被调度到跟当前节点和磁盘在同一个可用区，如果 集群中同一个可用区的节点不满足调度条件，即使其它可用区节点各种条件都满足，但不跟当前节点在 同一个可用区，也是不会调度的。为什么需要限制挂载了磁盘的 Pod 不能漂移到其它可用区的节点？ 试想一下，云上的磁盘虽然可以被动态挂载到不同机器，但也只是相对同一个<a href="https://so.csdn.net/so/search?q=%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%BF%83&amp;spm=1001.2101.3001.7020" target="_blank" rel="noopener noreferrer">数据中心</a>，通常不允许跨数据中心挂载磁盘设备，因为网络时延会极大的降低 IO 速率。</p><h3 id="_2、pod-一直处于-containercreating-或-waiting-状-态" tabindex="-1"><a class="header-anchor" href="#_2、pod-一直处于-containercreating-或-waiting-状-态"><span>2、Pod 一直处于 ContainerCreating 或 Waiting 状 态</span></a></h3><ul><li><strong>Pod 配置错误</strong><ul><li>检查是否打包了正确的镜像</li><li>检查配置了正确的容器参数</li></ul></li><li><strong>挂载 Volume 失败</strong><ul><li>Volume 挂载失败也分许多种情况，先列下我这里目前已知的。</li></ul></li></ul><p><strong>Pod 漂移没有正常解挂之前的磁盘</strong></p><blockquote><p>在云尝试托管的 <a href="https://so.csdn.net/so/search?q=K8S&amp;spm=1001.2101.3001.7020" target="_blank" rel="noopener noreferrer">K8S</a> 服务环境下，默认挂载的 Volume 一般是块存储类型的云硬盘，如果某个节点 挂了，kubelet 无法正常运行或与 apiserver 通信，到达时间阀值后会触发驱逐，自动在其它节点 上启动相同的副本 (Pod 漂移)，但是由于被驱逐的 Node 无法正常运行并不知道自己被驱逐了，也 就没有正常执行解挂，cloud-controller-manager 也在等解挂成功后再调用云厂商的接口将磁盘 真正从节点上解挂，通常会等到一个时间阀值后 cloud-controller-manager 会强制解挂云盘， 然后再将其挂载到 Pod 最新所在节点上，这种情况下 ContainerCreating 的时间相对长一点，但 一般最终是可以启动成功的，除非云厂商的 cloud-controller-manager 逻辑有 bug。</p></blockquote><p><strong>磁盘爆满</strong><br> 启动 Pod 会调 CRI 接口创建容器，容器运行时创建容器时通常会在数据目录下为新建的容器创建一 些目录和文件，如果数据目录所在的磁盘空间满了就会创建失败并报错:</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" data-title="" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span>Events：</span></span>\n<span class="line"><span>    Type Reason Age From</span></span>\n<span class="line"><span>Message</span></span>\n<span class="line"><span>     ---- ------ ---- ----</span></span>\n<span class="line"><span> Warning FailedCreatePodSandBox 2m (x4307 over 16h) kubelet, 10.179.80.31 (combined from similar events): Failed create pod sandbox: rpc error: code = Unknown desc = failed to create a sandbox for pod &quot;apigateway-6dc48bf8b6-l8xrw&quot;: Error response from daemon: mkdir /var/lib/docker/aufs/mnt/1f09d6c1c9f24e8daaea5bf33a4230de7dbc758e3b22785e8ee21e3e3d921214 no space left on device</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>处理方法: <a href="https://blog.csdn.net/qq_31055683/article/details/126974456" target="_blank" rel="noopener noreferrer">https://blog.csdn.net/qq_31055683/article/details/126974456</a><br><strong>节点内存碎片化</strong><br> 如果节点上内存碎片化严重，缺少大页内存，会导致即使总的剩余内存较多，但还是会申请内存失败，处理方法请看：<a href="https://blog.csdn.net/qq_31055683/article/details/126974873" target="_blank" rel="noopener noreferrer">https://blog.csdn.net/qq_31055683/article/details/12697487</a><br><strong>limit 设置太小或者单位不对</strong><br> 如果 limit 设置过小以至于不足以成功运行 Sandbox 也会造成这种状态，常见的是因为 memory limit 单位设置不对造成的 limit 过小，比如误将 memory 的 limit 单位像 request 一样设 置为小 <strong>m</strong> ，这个单位在 memory 不适用，会被 k8s 识别成 byte， 应该用 <strong>Mi</strong> 或<strong>M</strong><br><em>举个例子: 如果 memory limit 设为 1024m 表示限制 1.024 Byte，这么小的内存， pause 容器一起来就会被 cgroup-oom kill 掉，导致 pod 状态一直处于 ContainerCreating。</em><br> 这种情况通常会报下面的 event:</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" data-title="" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span>Pod sandbox changed, it will be killed and re-created。</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>kubelet 报错:</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" data-title="" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span>to start sandbox container for pod ... Error response from daemon: OCI runtime create failed: container_linux.go:348: starting container process caused &quot;process_linux.go:301: running exec setns process for init caused \\&quot;signal: killed\\&quot;&quot;: unknown</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p><strong>拉取镜像失败</strong><br> 镜像拉取失败也分很多情况，这里列举下:</p><ul><li>配置了错误的镜像</li><li>Kubelet 无法访问镜像仓库（比如默认 pause 镜像在 <a href="http://gcr.io" target="_blank" rel="noopener noreferrer">gcr.io</a> 上，国内环境访问需要特殊处 理）</li><li>拉取私有镜像的 imagePullSecret 没有配置或配置有误</li><li>镜像太大，拉取超时（可以适当调整 kubelet 的 —image-pull-progress-deadline 和 —runtime-request-timeout 选项）</li></ul><p><strong>CNI 网络错误</strong><br> 如果发生 CNI 网络错误通常需要检查下网络插件的配置和运行状态，如果没有正确配置或正常运行通 常表现为:</p><ul><li>无法配置 Pod 网络</li><li>无法分配 Pod IP</li></ul><p><strong>controller-manager 异常</strong><br> 查看 master 上 kube-controller-manager 状态，异常的话尝试重启。<br><strong>安装 docker 没删干净旧版本</strong></p><p>如果节点上本身有 docker 或者没删干净，然后又安装 docker，比如在 centos 上用 yum 安装:</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" data-title="" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span>yum install -y docker</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>这样可能会导致 dockerd 创建容器一直不成功，从而 Pod 状态一直 ContainerCreating，查看 event 报错:</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" data-title="" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span>Type     Reason        Age       From</span></span>\n<span class="line"><span>Message</span></span>\n<span class="line"><span>      ----    ------       ----       ----</span></span>\n<span class="line"><span>      Warning FailedCreatePodSandBox 18m (x3583 over 83m) kubelet, 192.168.4.5 (combined from similar events): Failed create pod sandbox: rpc error: code = Unknown desc = failed to start sandbox container for pod &quot;nginx-7db9fccd9b-2j6dh&quot;: Error response from daemon: ttrpc: client shutting down: read unix @-&gt;@/containerd- shim/moby/de2bfeefc999af42783115acca62745e6798981dff75f4148fae8c086668f667/shim.sock</span></span>\n<span class="line"><span> read: connection reset by peer: unknown</span></span>\n<span class="line"><span> Normal SandboxChanged 3m12s (x4420 over 83m) kubelet, 192.168.4.5</span></span>\n<span class="line"><span> Pod sandbox changed, it will be killed and re-created</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>可能是因为重复安装 docker 版本不一致导致一些组件之间不兼容，从而导致 dockerd 无法正常创 建容器。</p><ul><li><strong>存在同名容器</strong></li></ul><p>如果节点上已有同名容器，创建 sandbox 就会失败，event:</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" data-title="" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span>Warning FailedCreatePodSandBox 2m kubelet, 10.205.8.91 Failed create pod sandbox: rpc error: code = Unknown desc = failed to create a sandbox for pod &quot;lomp-ext-d8c8b8c46-4v8tl&quot;: operation timeout: context deadline exceeded</span></span>\n<span class="line"><span>Warning FailedCreatePodSandBox 3s (x12 over 2m) kubelet, 10.205.8.91 Failed create pod sandbox: rpc error: code = Unknown desc = failed to create a sandbox for pod &quot;lomp-ext-d8c8b8c46-4v8tl&quot;: Error response from daemon: Conflict. The container name &quot;/k8s_POD_lomp-ext-d8c8b8c46- 4v8tl_default_65046a06-f795-11e9-9bb6-b67fb7a70bad_0&quot; is already in use by container &quot;30aa3f5847e0ce89e9d411e76783ba14accba7eb7743e605a10a9a862a72c1e2&quot;. You have to remove (or rename) that container to be able to reuse that name.</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div></div></div><p>关于什么情况下会产生同名容器，这个有待研究。</p><h3 id="_3、pod-处于-crashloopbackoff-状态" tabindex="-1"><a class="header-anchor" href="#_3、pod-处于-crashloopbackoff-状态"><span>3、Pod 处于 CrashLoopBackOff 状态</span></a></h3><p>Pod 如果处于 CrashLoopBackOff 状态说明之前是启动了，只是又异常退出了，只要 Pod 的 restartPolicy不是 Never 就可能被重启拉起，此时 Pod 的 RestartCounts 通常是大于 0 的，可以先看下容器进程的退出状态码来缩小问题范围</p><ul><li><strong>容器进程主动退出：</strong></li></ul><hr><p>如果是容器进程主动退出，退出状态码一般在 0-128 之间，除了可能是业务程序 BUG，还有其它许 多可能原因</p><ul><li><strong>系统OOM</strong></li></ul><hr><p>如果发生系统 OOM，可以看到 Pod 中容器退出状态码是 137，表示被 SIGKILL 信号杀死，同时 内核会报错: Out of memory: Kill process … 。大概率是节点上部署了其它非 K8S 管理的进 程消耗了比较多的内存，或者 kubelet 的 --kube-reserved 和 --system-reserved 配的 比较小，没有预留足够的空间给其它非容器进程，节点上所有 Pod 的实际内存占用总量不会超过 /sys/fs/cgroup/memory/kubepods 这里 cgroup 的限制，这个限制等于 capacity - &quot;kube- reserved&quot; - &quot;system-reserved&quot; ，如果预留空间设置合理，节点上其它非容器进程（kubelet, dockerd, kube-proxy, sshd 等) 内存占用没有超过 kubelet 配置的预留空间是不会发生系统 OOM 的，可以根据实际需求做合理的调整。</p><ul><li><strong>系统OOM</strong></li></ul><hr><p>如果是 cgrou OOM 杀掉的进程，从 Pod 事件的下 Reason 可以看到是 OOMKilled ，说明 容器实际占用的内存超过 limit 了，同时内核日志会报: ``。 可以根据需求调整下 limit。</p><ul><li><strong>节点内存碎片化</strong></li></ul><hr><p>如果节点上内存碎片化严重，缺少大页内存，会导致即使总的剩余内存较多，但还是会申请内存失败，</p><ul><li><strong>健康检查失败</strong></li></ul><hr><h3 id="_4、pod-一直处于-terminating-状态" tabindex="-1"><a class="header-anchor" href="#_4、pod-一直处于-terminating-状态"><span>4、Pod 一直处于 Terminating 状态</span></a></h3><ul><li><strong>磁盘爆满</strong></li></ul><hr><p>如果 docker 的数据目录所在磁盘被写满，docker 无法正常运行，无法进行删除和创建操作，所以 kubelet 调用 docker 删除容器没反应，看 event 类似这样：</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" data-title="" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span>Normal Killing 39s (x735 over 15h) kubelet, 10.179.80.31 Killing container with id docker://apigateway:Need to kill Pod</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><ul><li><strong>存在 “i” 文件属性</strong></li></ul><hr><p>如果容器的镜像本身或者容器启动后写入的文件存在 “i” 文件属性，此文件就无法被修改删除，而删 除 Pod 时会清理容器目录，但里面包含有不可删除的文件，就一直删不了，Pod 状态也将一直保持 Terminating，kubelet 报错:</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" data-title="" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span>Sep 27 14:37:21 VM_0_7_centos kubelet[14109]: E0927 14:37:21.922965 14109 remote_runtime RemoveContainer &quot;19d837c77a3c294052a99ff9347c520bc8acb7b8b9a9dc9fab281fc09df38257&quot; failed: rpc error: code = Unknown desc = failed to remove container &quot;19d837c77a3c294052a99ff9347c520bc8acb7b8b9a9dc9fab281fc09df38257&quot;: Error response 19d837c77a3c294052a99ff9347c520bc8acb7b8b9a9dc9fab281fc09df38257: driver &quot;overlay2&quot; filesystem: remove /data/docker/overlay2/b1aea29c590aa9abda79f7cf3976422073fb3652757f0391db88534027546868 operation not permitted</span></span>\n<span class="line"><span>Sep 27 14:37:21 VM_0_7_centos kubelet[14109]: E0927 14:37:21.923027 14109 kuberuntime_gc to remove container &quot;19d837c77a3c294052a99ff9347c520bc8acb7b8b9a9dc9fab281fc09df38257&quot; Unknown desc = failed to remove container &quot;19d837c77a3c294052a99ff9347c520bc8acb7b8b9a9dc9fab281fc09df38257&quot;: Error response 19d837c77a3c294052a99ff9347c520bc8acb7b8b9a9dc9fab281fc09df38257: driver &quot;overlay2&quot; filesystem: remove /data/docker/overlay2/b1aea29c590aa9abda79f7cf3976422073fb3652757f0391db88534027546868 operation not permitted</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div></div></div><p>通过 <strong>man chattr</strong> 查看 “i” 文件属性描述:</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" data-title="" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span>A file with the &#39;i&#39; attribute cannot be modified: it cannot be deleted or renamed, no link can be created to this file and no data can be written to the file. Only the superuser or a process possessing the CAP_LINUX_IMMUTABLE capability can set or clear this attribute.</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>彻底解决当然是不要在容器镜像中或启动后的容器设置 “i” 文件属性，临时恢复方法： 复制 kubelet 日志报错提示的文件路径，然后执行 chattr -i 文件名 :</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" data-title="" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span>chattr -i /data/docker/overlay2/b1aea29c590aa9abda79f7cf3976422073fb3652757f0391db88534027</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>执行完后等待 kubelet 自动重试，Pod 就可以被自动删除了。</p><ul><li><strong>存在 Finalizers</strong></li></ul><hr><p>k8s 资源的 metadata 里如果存在 finalizers ，那么该资源一般是由某程序创建的，并且在其 创建的资源的 metadata 里的 finalizers 加了一个它的标识，这意味着这个资源被删除时需要 由创建资源的程序来做删除前的清理，清理完了它需要将标识从该资源的 finalizers 中移除，然 后才会最终彻底删除资源。比如 Rancher 创建的一些资源就会写入 finalizers 标识。<br> 处理建议： kubectl edit 手动编辑资源定义，删掉 finalizers ，这时再看下资源，就会发现 已经删掉了</p><h3 id="_5、pod-一直处于-unknown-状态" tabindex="-1"><a class="header-anchor" href="#_5、pod-一直处于-unknown-状态"><span>5、Pod 一直处于 Unknown 状态</span></a></h3><p>通常是节点失联，没有上报状态给 apiserver，到达阀值后 controller-manager 认为节点失联 并将其状态置为 Unknown<br> 可能原因:</p><ul><li>节点高负载导致无法上报</li><li>节点宕机</li><li>节点被关机</li><li>网络不通</li></ul><h3 id="_6、pod-一直处于-error-状态" tabindex="-1"><a class="header-anchor" href="#_6、pod-一直处于-error-状态"><span>6、Pod 一直处于 Error 状态</span></a></h3><p>通常处于 Error 状态说明 Pod 启动过程中发生了错误。常见的原因包括：</p><ul><li>依赖的 ConfigMap、Secret 或者 PV 等不存在</li><li>请求的资源超过了管理员设置的限制，比如超过了 LimitRange 等</li><li>违反集群的安全策略，比如违反了 PodSecurityPolicy 等</li><li>容器无权操作集群内的资源，比如开启 RBAC 后，需要为 ServiceAccount 配置角色绑定</li></ul><h3 id="_7、pod-一直处于-imagepullbackoff-状态" tabindex="-1"><a class="header-anchor" href="#_7、pod-一直处于-imagepullbackoff-状态"><span>7、Pod 一直处于 ImagePullBackOff 状态</span></a></h3><ul><li><strong>http 类型 registry，地址未加入到 insecure- registry</strong></li></ul><hr><p>dockerd 默认从 https 类型的 registry 拉取镜像，如果使用 https 类型的 registry，则 必须将它添加到 insecure-registry 参数中，然后重启或 reload dockerd 生效。</p><ul><li><strong>https 自签发类型 resitry，没有给节点添加 ca 证书</strong></li></ul><hr><p>如果 registry 是 https 类型，但证书是自签发的，dockerd 会校验 registry 的证书，校验 成功才能正常使用镜像仓库，要想校验成功就需要将 registry 的 ca 证书放置到<br> /etc/docker/certs.d/<a href="registry:port" target="_blank" rel="noopener noreferrer">registry:port</a>/ca.crt 位置</p><ul><li><strong>私有镜像仓库认证失败</strong></li></ul><hr><p>如果 registry 需要认证，但是 Pod 没有配置 imagePullSecret，配置的 Secret 不存在或者 有误都会认证失败。</p><ul><li><strong>镜像文件损坏</strong></li></ul><hr><p>如果 push 的镜像文件损坏了，下载下来也用不了，需要重新 push 镜像文件。</p><ul><li><strong>镜像拉取超时</strong></li></ul><hr><p>如果节点上新起的 Pod 太多就会有许多可能会造成容器镜像下载排队，如果前面有许多大镜像需要下 载很长时间，后面排队的 Pod 就会报拉取超时。<br> kubelet 默认串行下载镜像:</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" data-title="" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span>--serialize-image-pulls Pull images one at a time. We recommend *not* changing the default value on nodes that run docker daemon with version &lt; 1.9 or an Aufs storage backend. Issue #10959 has more details. (default true)</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>也可以开启并行下载并控制并发:</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" data-title="" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span>--registry-qps int32 If &gt; 0, limit registry pull QPS to this value. If 0, unlimited. (default 5)</span></span>\n<span class="line"><span>--registry-burst int32 Maximum size of a bursty pulls, temporarily allows pulls to burst to this number, while still not exceeding registry-qps. Only used if --registry-qps &gt; 0 (default 10)</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div></div></div><ul><li><strong>镜像不存在</strong></li></ul><hr><p>kubelet日志：</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" data-title="" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span>PullImage &quot;imroc/test:v0.2&quot; from image service failed: rpc error: code = Unknown desc = Error response from daemon: manifest for imroc/test:v0.2 not found</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><h3 id="_8、pod-健康检查失败" tabindex="-1"><a class="header-anchor" href="#_8、pod-健康检查失败"><span>8、Pod 健康检查失败</span></a></h3><ul><li>Kubernetes 健康检查包含就绪检查(readinessProbe)和存活检查(livenessProbe)</li><li>pod 如果就绪检查失败会将此 pod ip 从 service 中摘除，通过 service 访问，流量将 不会被转发给就绪检查失败的 pod</li><li>pod 如果存活检查失败，kubelet 将会杀死容器并尝试重启</li></ul><p>健康检查失败的可能原因有多种，除了业务程序BUG导致不能响应健康检查导致 unhealthy，还能有 有其它原因，下面我们来逐个排查。</p><ul><li><strong>健康检查配置不合理</strong></li></ul><hr><p>initialDelaySeconds 太短，容器启动慢，导致容器还没完全启动就开始探测，如果 successThreshold 是默认值 1，检查失败一次就会被 kill，然后 pod 一直这样被 kill 重 启。</p><ul><li><strong>节点负载过高</strong></li></ul><hr><p>cpu 占用高（比如跑满）会导致进程无法正常发包收包，通常会 timeout，导致 kubelet 认为 pod 不健康</p><ul><li><strong>容器进程被木马进程杀死</strong></li></ul><hr><ul><li><strong>容器内进程端口监听挂掉</strong></li></ul><hr><p>使用 netstat -tunlp 检查端口监听是否还在，如果不在了，抓包可以看到会直接 reset 掉健 康检查探测的连接:</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" data-title="" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span>20:15:17.890996 IP 172.16.2.1.38074 &gt; 172.16.2.23.8888: Flags [S], seq 96880261, win 14600, options [mss 1424,nop,nop,sackOK,nop,wscale 7], length 0</span></span>\n<span class="line"><span>20:15:17.891021 IP 172.16.2.23.8888 &gt; 172.16.2.1.38074: Flags [R.], seq 0, ack 96880262, win 0, length 0</span></span>\n<span class="line"><span>20:15:17.906744 IP 10.0.0.16.54132 &gt; 172.16.2.23.8888: Flags [S], seq 1207014342, win 14600, options [mss 1424,nop,nop,sackOK,nop,wscale 7], length 0</span></span>\n<span class="line"><span>20:15:17.906766 IP 172.16.2.23.8888 &gt; 10.0.0.16.54132: Flags [R.], seq 0, ack 1207014343, win 0, length 0</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>连接异常，从而健康检查失败。发生这种情况的原因可能在一个节点上启动了多个使用 hostNetwork 监听相同宿主机端口的 Pod，只会有一个 Pod 监听成功，但监听失败的 Pod 的 业务逻辑允许了监听失败，并没有退出，Pod 又配了健康检查，kubelet 就会给 Pod 发送健康检查 探测报文，但 Pod 由于没有监听所以就会健康检查失败。</p><ul><li><strong>SYN backlog 设置过小</strong></li></ul><hr><p>SYN backlog 大小即 SYN 队列大小，如果短时间内新建连接比较多，而 SYN backlog 设置太 小，就会导致新建连接失败，通过 netstat -s | grep TCPBacklogDrop可以看到有多少是因为 backlog 满了导致丢弃的新连接。<br> 如果确认是 backlog 满了导致的丢包，建议调高 backlog 的值，内核参数为 net.ipv4.tcp_max_syn_backlog 。</p><ul><li><strong>容器进程主动退出</strong></li></ul><hr><p>容器进程如果是自己主动退出(不是被外界中断杀死)，退出状态码一般在 0-128 之间，根据约定，正 常退出时状态码为 0，1-127 说明是程序发生异常，主动退出了，比如检测到启动的参数和条件不满 足要求，或者运行过程中发生 panic 但没有捕获处理导致程序退出。除了可能是业务程序 BUG，还 有其它许多可能原因，这里我们一一列举下。</p><ul><li><strong>DNS 无法解析</strong><br> 可能程序依赖 集群 DNS 服务，比如启动时连接数据库，数据库使用 service 名称或外部域名都需 要 DNS 解析，如果解析失败程序将报错并主动退出。解析失败的可能原因:1、集群网络有问题，Pod 连不上集群 DNS 服务<br> 2、集群 DNS 服务挂了，无法响应解析请求<br> 3、Service 或域名地址配置有误，本身是无法解析的地址</li><li><strong>程序配置有误</strong><br> 1、配置文件格式错误，程序启动解析配置失败报错退出<br> 2、配置内容不符合规范，比如配置中某个字段是必选但没有填写，配置校验不通过，程序报错主动 退出</li></ul><p>参考：<br><a href="https://blog.csdn.net/qq_31055683/article/details/126970831" target="_blank" rel="noopener noreferrer">https://blog.csdn.net/qq_31055683/article/details/126970831</a></p>',139)],l={},r=(0,n(6262).A)(l,[["render",function(e,i){return(0,a.uX)(),(0,a.CE)("div",null,s)}]]),t=JSON.parse('{"path":"/ContainerCloud/k8s/act_none_pod%E5%BC%82%E5%B8%B8%E7%8A%B6%E6%80%81%E6%8E%92%E6%9F%A5.html","title":"2、Pod异常状态排错","lang":"zh-CN","frontmatter":{"title":"2、Pod异常状态排错","icon":"file","order":2,"author":"bugcode","date":"2020-01-01T00:00:00.000Z","category":["K8S","DOCKER","JAVA"],"tag":["DOCKER","云原生","K8S"],"sticky":false,"star":true,"footer":"云原生","copyright":"bugcode","description":"2、Pod异常状态排错 一、常用命令 首先列出Pod排查过程中我这边的常用命令： 查看Pod状态：kubectl get pod podname -o wide 查看Pod的yaml配置：kubectl get pods podname -o yaml 查看pod事件：kubectl describe pods podname 查看容器日志：kubec...","head":[["meta",{"property":"og:url","content":"https://www.bugcode.online/ContainerCloud/k8s/act_none_pod%E5%BC%82%E5%B8%B8%E7%8A%B6%E6%80%81%E6%8E%92%E6%9F%A5.html"}],["meta",{"property":"og:site_name","content":"bugcode 的架构之路"}],["meta",{"property":"og:title","content":"2、Pod异常状态排错"}],["meta",{"property":"og:description","content":"2、Pod异常状态排错 一、常用命令 首先列出Pod排查过程中我这边的常用命令： 查看Pod状态：kubectl get pod podname -o wide 查看Pod的yaml配置：kubectl get pods podname -o yaml 查看pod事件：kubectl describe pods podname 查看容器日志：kubec..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2024-08-12T08:08:43.000Z"}],["meta",{"property":"article:author","content":"bugcode"}],["meta",{"property":"article:tag","content":"DOCKER"}],["meta",{"property":"article:tag","content":"云原生"}],["meta",{"property":"article:tag","content":"K8S"}],["meta",{"property":"article:published_time","content":"2020-01-01T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2024-08-12T08:08:43.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"2、Pod异常状态排错\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2020-01-01T00:00:00.000Z\\",\\"dateModified\\":\\"2024-08-12T08:08:43.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"bugcode\\"}]}"]]},"headers":[{"level":2,"title":"一、常用命令","slug":"一、常用命令","link":"#一、常用命令","children":[]},{"level":2,"title":"二、Pod状态","slug":"二、pod状态","link":"#二、pod状态","children":[]},{"level":2,"title":"三、pod遇到的问题","slug":"三、pod遇到的问题","link":"#三、pod遇到的问题","children":[{"level":3,"title":"1、pod一直处于Pending状态","slug":"_1、pod一直处于pending状态","link":"#_1、pod一直处于pending状态","children":[]},{"level":3,"title":"2、Pod 一直处于 ContainerCreating 或 Waiting 状 态","slug":"_2、pod-一直处于-containercreating-或-waiting-状-态","link":"#_2、pod-一直处于-containercreating-或-waiting-状-态","children":[]},{"level":3,"title":"3、Pod 处于 CrashLoopBackOff 状态","slug":"_3、pod-处于-crashloopbackoff-状态","link":"#_3、pod-处于-crashloopbackoff-状态","children":[]},{"level":3,"title":"4、Pod 一直处于 Terminating 状态","slug":"_4、pod-一直处于-terminating-状态","link":"#_4、pod-一直处于-terminating-状态","children":[]},{"level":3,"title":"5、Pod 一直处于 Unknown 状态","slug":"_5、pod-一直处于-unknown-状态","link":"#_5、pod-一直处于-unknown-状态","children":[]},{"level":3,"title":"6、Pod 一直处于 Error 状态","slug":"_6、pod-一直处于-error-状态","link":"#_6、pod-一直处于-error-状态","children":[]},{"level":3,"title":"7、Pod 一直处于 ImagePullBackOff 状态","slug":"_7、pod-一直处于-imagepullbackoff-状态","link":"#_7、pod-一直处于-imagepullbackoff-状态","children":[]},{"level":3,"title":"8、Pod 健康检查失败","slug":"_8、pod-健康检查失败","link":"#_8、pod-健康检查失败","children":[]}]}],"git":{"createdTime":1723450123000,"updatedTime":1723450123000,"contributors":[{"name":"不爱打代码的程序员","email":"2284609302@qq.com","commits":1}]},"readingTime":{"minutes":18.84,"words":5653},"filePathRelative":"ContainerCloud/k8s/act_none_pod异常状态排查.md","localizedDate":"2020年1月1日","excerpt":"<p>2、Pod异常状态排错</p>\\n<h2>一、常用命令</h2>\\n<p>首先列出Pod排查过程中我这边的常用命令：</p>\\n<ul>\\n<li>查看Pod状态：kubectl get pod podname -o wide</li>\\n<li>查看Pod的yaml配置：kubectl get pods podname -o yaml</li>\\n<li>查看pod事件：kubectl describe pods podname</li>\\n<li>查看容器日志：kubectl logs podsname -c container-name</li>\\n</ul>\\n<h2>二、Pod状态</h2>\\n<ul>\\n<li>Error：Pod 启动过程中发生错误</li>\\n<li>NodeLost : Pod 所在节点失联</li>\\n<li>Unkown : Pod 所在节点失联或其它未知异常</li>\\n<li>Waiting : Pod 等待启动</li>\\n<li>Pending : Pod 等待被调度</li>\\n<li>ContainerCreating : Pod 容器正在被创建</li>\\n<li>Terminating : Pod 正在被销毁</li>\\n<li>CrashLoopBackOff ： 容器退出，kubelet 正在将它重启</li>\\n<li>InvalidImageName ： 无法解析镜像名称</li>\\n<li>ImageInspectError ： 无法校验镜像</li>\\n<li>ErrImageNeverPull ： 策略禁止拉取镜像</li>\\n<li>ImagePullBackOff ： 正在重试拉取</li>\\n<li>RegistryUnavailable ： 连接不到镜像中心</li>\\n<li>ErrImagePull ： 通用的拉取镜像出错</li>\\n<li>CreateContainerConfigError ： 不能创建 kubelet 使用的容器配置</li>\\n<li>CreateContainerError ： 创建容器失败</li>\\n<li>RunContainerError ： 启动容器失败</li>\\n<li>PreStartHookError : 执行 preStart hook 报错</li>\\n<li>PostStartHookError ： 执行 postStart hook 报错</li>\\n<li>ContainersNotInitialized ： 容器没有初始化完毕</li>\\n<li>ContainersNotReady ： 容器没有准备完毕</li>\\n<li>ContainerCreating ：容器创建中</li>\\n<li>PodInitializing ：pod 初始化中</li>\\n<li>DockerDaemonNotReady ：docker还没有完全启动</li>\\n<li>NetworkPluginNotReady ： 网络插件还没有完全启动</li>\\n</ul>","autoDesc":true}')}}]);